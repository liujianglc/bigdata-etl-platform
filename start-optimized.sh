#!/bin/bash

# ä¼˜åŒ–çš„ä¸€æ¬¡æ€§å¯åŠ¨è„šæœ¬
# æ”¯æŒå®Œæ•´çš„æœåŠ¡å¯åŠ¨å’Œæ•°æ®åˆå§‹åŒ–

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_step() {
    echo -e "${BLUE}[STEP]${NC} $1"
}

# æ£€æŸ¥å¿…è¦æ–‡ä»¶
check_requirements() {
    log_step "æ£€æŸ¥å¿…è¦æ–‡ä»¶..."
    
    local required_files=(
        ".env"
        "docker-compose.yaml"
        "Dockerfile.airflow"
        "setup-connections-enhanced.py"
        "init-hdfs-directories.sh"
        "init-multiple-databases.sh"
        "init-mysql-data.sql"
    )
    
    for file in "${required_files[@]}"; do
        if [[ ! -f "$file" ]]; then
            log_error "å¿…è¦æ–‡ä»¶ä¸å­˜åœ¨: $file"
            exit 1
        fi
    done
    
    log_info "æ‰€æœ‰å¿…è¦æ–‡ä»¶æ£€æŸ¥å®Œæˆ"
}

# è®¾ç½®æƒé™
set_permissions() {
    log_step "è®¾ç½®è„šæœ¬æƒé™..."
    
    chmod +x init-hdfs-directories.sh
    chmod +x init-multiple-databases.sh
    chmod +x setup-connections-enhanced.py
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    mkdir -p logs dags config plugins spark_jobs
    
    # è®¾ç½®ç›®å½•æƒé™
    chmod -R 755 logs dags config plugins spark_jobs
    
    log_info "æƒé™è®¾ç½®å®Œæˆ"
}

# æ¸…ç†æ—§å®¹å™¨å’Œæ•°æ®
cleanup_old_resources() {
    log_step "æ¸…ç†æ—§çš„å®¹å™¨å’Œç½‘ç»œ..."
    
    # åœæ­¢å¹¶åˆ é™¤æ—§å®¹å™¨
    docker-compose down -v --remove-orphans 2>/dev/null || true
    
    # åˆ é™¤æ‚¬ç©ºçš„é•œåƒ
    docker image prune -f 2>/dev/null || true
    
    log_info "æ¸…ç†å®Œæˆ"
}

# æ„å»ºé•œåƒ
build_images() {
    log_step "æ„å»ºAirflowé•œåƒ..."
    
    docker-compose build airflow-webserver
    
    if [[ $? -eq 0 ]]; then
        log_info "é•œåƒæ„å»ºå®Œæˆ"
    else
        log_error "é•œåƒæ„å»ºå¤±è´¥"
        exit 1
    fi
}

# å¯åŠ¨åŸºç¡€æœåŠ¡
start_base_services() {
    log_step "å¯åŠ¨åŸºç¡€æœåŠ¡ (PostgreSQL, Redis, MySQL)..."
    
    docker-compose up -d postgres redis mysql
    
    # ç­‰å¾…åŸºç¡€æœåŠ¡å¯åŠ¨
    log_info "ç­‰å¾…åŸºç¡€æœåŠ¡å¯åŠ¨..."
    sleep 30
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    local services=("postgres" "redis" "mysql")
    for service in "${services[@]}"; do
        if docker-compose ps "$service" | grep -q "Up"; then
            log_info "$service æœåŠ¡å¯åŠ¨æˆåŠŸ"
        else
            log_error "$service æœåŠ¡å¯åŠ¨å¤±è´¥"
            docker-compose logs "$service"
            exit 1
        fi
    done
}

# å¯åŠ¨å¤§æ•°æ®æœåŠ¡
start_bigdata_services() {
    log_step "å¯åŠ¨å¤§æ•°æ®æœåŠ¡ (HDFS, Hive, Spark)..."
    
    # å¯åŠ¨HDFS
    docker-compose up -d namenode datanode
    log_info "ç­‰å¾…HDFSå¯åŠ¨..."
    sleep 45
    
    # å¯åŠ¨Hive Metastore
    docker-compose up -d hive-metastore
    log_info "ç­‰å¾…Hive Metastoreå¯åŠ¨..."
    sleep 30
    
    # å¯åŠ¨Spark
    docker-compose up -d spark-master spark-worker
    log_info "ç­‰å¾…Sparkå¯åŠ¨..."
    sleep 20
    
    # æ£€æŸ¥å¤§æ•°æ®æœåŠ¡çŠ¶æ€
    local bigdata_services=("namenode" "datanode" "hive-metastore" "spark-master" "spark-worker")
    for service in "${bigdata_services[@]}"; do
        if docker-compose ps "$service" | grep -q "Up"; then
            log_info "$service æœåŠ¡å¯åŠ¨æˆåŠŸ"
        else
            log_warn "$service æœåŠ¡å¯èƒ½å¯åŠ¨å¤±è´¥ï¼Œæ£€æŸ¥æ—¥å¿—..."
            docker-compose logs --tail=20 "$service"
        fi
    done
}

# åˆå§‹åŒ–æ•°æ®
initialize_data() {
    log_step "åˆå§‹åŒ–æ•°æ®å’Œç›®å½•ç»“æ„..."
    
    # è¿è¡ŒHDFSç›®å½•åˆå§‹åŒ–
    log_info "åˆå§‹åŒ–HDFSç›®å½•..."
    docker-compose up hdfs-init
    
    if [[ $? -eq 0 ]]; then
        log_info "HDFSç›®å½•åˆå§‹åŒ–å®Œæˆ"
    else
        log_warn "HDFSç›®å½•åˆå§‹åŒ–å¯èƒ½å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ..."
    fi
}

# å¯åŠ¨AirflowæœåŠ¡
start_airflow_services() {
    log_step "å¯åŠ¨AirflowæœåŠ¡..."
    
    # è¿è¡ŒAirflowåˆå§‹åŒ–
    log_info "åˆå§‹åŒ–Airflow..."
    docker-compose up airflow-init
    
    if [[ $? -eq 0 ]]; then
        log_info "Airflowåˆå§‹åŒ–å®Œæˆ"
    else
        log_error "Airflowåˆå§‹åŒ–å¤±è´¥"
        docker-compose logs airflow-init
        exit 1
    fi
    
    # å¯åŠ¨AirflowæœåŠ¡
    log_info "å¯åŠ¨AirflowæœåŠ¡..."
    docker-compose up -d airflow-webserver airflow-scheduler airflow-worker
    
    # ç­‰å¾…Airflowå¯åŠ¨
    log_info "ç­‰å¾…AirflowæœåŠ¡å¯åŠ¨..."
    sleep 60
    
    # æ£€æŸ¥AirflowæœåŠ¡çŠ¶æ€
    local airflow_services=("airflow-webserver" "airflow-scheduler" "airflow-worker")
    for service in "${airflow_services[@]}"; do
        if docker-compose ps "$service" | grep -q "Up"; then
            log_info "$service æœåŠ¡å¯åŠ¨æˆåŠŸ"
        else
            log_error "$service æœåŠ¡å¯åŠ¨å¤±è´¥"
            docker-compose logs --tail=30 "$service"
        fi
    done
}

# éªŒè¯æœåŠ¡çŠ¶æ€
verify_services() {
    log_step "éªŒè¯æ‰€æœ‰æœåŠ¡çŠ¶æ€..."
    
    # æ£€æŸ¥ç«¯å£å¯è®¿é—®æ€§
    local ports=(
        "5432:PostgreSQL"
        "6379:Redis"
        "3306:MySQL"
        "9870:HDFS NameNode UI"
        "8081:Spark Master UI"
        "9083:Hive Metastore"
        "8080:Airflow WebUI"
    )
    
    for port_info in "${ports[@]}"; do
        local port=$(echo $port_info | cut -d: -f1)
        local service=$(echo $port_info | cut -d: -f2)
        
        if nc -z localhost "$port" 2>/dev/null; then
            log_info "$service (ç«¯å£ $port) å¯è®¿é—®"
        else
            log_warn "$service (ç«¯å£ $port) ä¸å¯è®¿é—®"
        fi
    done
    
    # æ˜¾ç¤ºæœåŠ¡çŠ¶æ€
    echo
    log_step "æ‰€æœ‰æœåŠ¡çŠ¶æ€:"
    docker-compose ps
}

# æ˜¾ç¤ºè®¿é—®ä¿¡æ¯
show_access_info() {
    log_step "æœåŠ¡è®¿é—®ä¿¡æ¯:"
    
    echo
    echo "ğŸŒ Webç•Œé¢è®¿é—®åœ°å€:"
    echo "  â€¢ Airflow WebUI:     http://localhost:8080 (admin/admin123)"
    echo "  â€¢ Spark Master UI:   http://localhost:8081"
    echo "  â€¢ HDFS NameNode UI:  http://localhost:9870"
    echo
    echo "ğŸ”Œ æ•°æ®åº“è¿æ¥ä¿¡æ¯:"
    echo "  â€¢ PostgreSQL:        localhost:5432 (airflow/airflow123)"
    echo "  â€¢ MySQL:             localhost:3306 (etl_user/etl_pass)"
    echo "  â€¢ Hive Metastore:    localhost:9083"
    echo
    echo "ğŸ“ é‡è¦ç›®å½•:"
    echo "  â€¢ DAGsç›®å½•:          ./dags/"
    echo "  â€¢ é…ç½®ç›®å½•:          ./config/"
    echo "  â€¢ Sparkä½œä¸šç›®å½•:     ./spark_jobs/"
    echo "  â€¢ æ—¥å¿—ç›®å½•:          ./logs/"
    echo
    echo "ğŸš€ å¿«é€Ÿå‘½ä»¤:"
    echo "  â€¢ æŸ¥çœ‹æ—¥å¿—:          docker-compose logs -f [service_name]"
    echo "  â€¢ é‡å¯æœåŠ¡:          docker-compose restart [service_name]"
    echo "  â€¢ åœæ­¢æ‰€æœ‰æœåŠ¡:      docker-compose down"
    echo "  â€¢ æŸ¥çœ‹æœåŠ¡çŠ¶æ€:      docker-compose ps"
    echo
}

# ä¸»å‡½æ•°
main() {
    log_info "å¼€å§‹å¯åŠ¨ä¼˜åŒ–çš„å¤§æ•°æ®ETLå¹³å°..."
    echo
    
    # æ£€æŸ¥Dockerå’ŒDocker Compose
    if ! command -v docker &> /dev/null; then
        log_error "Dockeræœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
        exit 1
    fi
    
    if ! command -v docker-compose &> /dev/null; then
        log_error "Docker Composeæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
        exit 1
    fi
    
    # æ‰§è¡Œå¯åŠ¨æ­¥éª¤
    check_requirements
    set_permissions
    cleanup_old_resources
    build_images
    start_base_services
    start_bigdata_services
    initialize_data
    start_airflow_services
    verify_services
    show_access_info
    
    log_info "ğŸ‰ å¤§æ•°æ®ETLå¹³å°å¯åŠ¨å®Œæˆï¼"
    log_info "è¯·ç­‰å¾…1-2åˆ†é’Ÿè®©æ‰€æœ‰æœåŠ¡å®Œå…¨å¯åŠ¨ï¼Œç„¶åè®¿é—® http://localhost:8080 æŸ¥çœ‹Airflowç•Œé¢"
}

# æ•è·ä¸­æ–­ä¿¡å·
trap 'log_error "å¯åŠ¨è¿‡ç¨‹è¢«ä¸­æ–­"; exit 1' INT TERM

# è¿è¡Œä¸»å‡½æ•°
main "$@"