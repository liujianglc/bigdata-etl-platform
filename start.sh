#!/bin/bash

echo "å¯åŠ¨å®Œæ•´å¤§æ•°æ®ç®¡é“ç¯å¢ƒ..."

# æ£€æµ‹docker-composeå‘½ä»¤
if [ -n "$DOCKER_COMPOSE_CMD" ]; then
    # ä½¿ç”¨ä»quick-deploy.shä¼ é€’çš„å‘½ä»¤
    COMPOSE_CMD="$DOCKER_COMPOSE_CMD"
elif command -v docker-compose &> /dev/null; then
    COMPOSE_CMD="docker-compose"
elif command -v docker &> /dev/null && docker compose version &> /dev/null; then
    COMPOSE_CMD="docker compose"
    echo "ğŸ’¡ ä½¿ç”¨ Docker Compose V2"
else
    echo "âŒ æ— æ³•æ‰¾åˆ°å¯ç”¨çš„docker-composeå‘½ä»¤"
    echo "è¯·å®‰è£…Docker Composeæˆ–è¿è¡Œ: ./fix-permissions.sh"
    exit 1
fi

# åœæ­¢å¯èƒ½è¿è¡Œçš„æœåŠ¡
$COMPOSE_CMD down

# åˆ›å»ºå¿…è¦çš„ç›®å½•
mkdir -p logs config plugins dags spark_jobs

# åˆ†é˜¶æ®µå¯åŠ¨æœåŠ¡
echo "ç¬¬ä¸€é˜¶æ®µï¼šå¯åŠ¨åŸºç¡€æœåŠ¡..."
$COMPOSE_CMD up -d postgres redis mysql

echo "ç­‰å¾…åŸºç¡€æœåŠ¡å¯åŠ¨..."
sleep 20

# åˆå§‹åŒ–Airflowæ•°æ®åº“
echo "åˆå§‹åŒ–Airflowæ•°æ®åº“..."
$COMPOSE_CMD run --rm airflow-webserver airflow db init

# åˆ›å»ºAirflowç®¡ç†å‘˜ç”¨æˆ·
echo "åˆ›å»ºAirflowç®¡ç†å‘˜ç”¨æˆ·..."
$COMPOSE_CMD run --rm airflow-webserver airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin

echo "ç¬¬äºŒé˜¶æ®µï¼šå¯åŠ¨HDFSæœåŠ¡..."
$COMPOSE_CMD up -d namenode datanode

echo "ç­‰å¾…HDFSæœåŠ¡å¯åŠ¨..."
sleep 30

echo "ç¬¬ä¸‰é˜¶æ®µï¼šå¯åŠ¨Hiveå’ŒSparkæœåŠ¡..."
$COMPOSE_CMD up -d hive-metastore spark-master spark-worker

echo "ç­‰å¾…Hiveå’ŒSparkæœåŠ¡å¯åŠ¨..."
sleep 30

echo "ç¬¬å››é˜¶æ®µï¼šå¯åŠ¨AirflowæœåŠ¡..."
$COMPOSE_CMD up -d airflow-webserver airflow-scheduler airflow-worker

echo "ç­‰å¾…AirflowæœåŠ¡å¯åŠ¨..."
sleep 30

# åˆå§‹åŒ–MySQLæµ‹è¯•æ•°æ®
echo "åˆå§‹åŒ–MySQLæµ‹è¯•æ•°æ®..."
$COMPOSE_CMD exec -T mysql mysql -u root -prootpass source_db < init-mysql-data.sql

# è®¾ç½®Airflowè¿æ¥
echo "è®¾ç½®Airflowè¿æ¥..."
./setup-connections.sh

# åˆå§‹åŒ–HDFSç›®å½•
echo "åˆå§‹åŒ–HDFSç›®å½•ç»“æ„..."
$COMPOSE_CMD exec namenode hdfs dfs -mkdir -p /user/hive/warehouse || true
$COMPOSE_CMD exec namenode hdfs dfs -mkdir -p /tmp/spark-events || true
$COMPOSE_CMD exec namenode hdfs dfs -chmod -R 777 /user/hive/warehouse || true
$COMPOSE_CMD exec namenode hdfs dfs -chmod -R 777 /tmp || true

echo "ç¯å¢ƒå¯åŠ¨å®Œæˆ!"
echo ""
echo "=== è®¿é—®åœ°å€ ==="
echo "- Airflow Web UI: http://localhost:8080 (admin/admin)"
echo "- Spark Master UI: http://localhost:8081"
echo "- HDFS NameNode UI: http://localhost:9870"
echo "- MySQL: localhost:3306 (root/rootpass)"
echo ""
echo "=== å¯ç”¨çš„DAG ==="
echo "1. simple_mysql_etl - ç®€å•MySQL ETLæµç¨‹"
echo "2. complete_bigdata_etl - å®Œæ•´å¤§æ•°æ®ETLæµç¨‹"
echo "3. mysql_to_hive_etl - MySQLåˆ°Hiveçš„ETLæµç¨‹"
echo ""
echo "=== æµ‹è¯•æ­¥éª¤ ==="
echo "1. è®¿é—®Airflow UI"
echo "2. å¯ç”¨æƒ³è¦è¿è¡Œçš„DAG"
echo "3. æ‰‹åŠ¨è§¦å‘DAGæ‰§è¡Œ"
echo "4. æŸ¥çœ‹æ‰§è¡Œæ—¥å¿—å’Œç»“æœ"