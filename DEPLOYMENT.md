# éƒ¨ç½²æŒ‡å—

## ğŸš€ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### 1. ç¯å¢ƒå‡†å¤‡

#### ç³»ç»Ÿè¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Linux (Ubuntu 20.04+ æ¨è)
- **å†…å­˜**: æœ€å°‘8GBï¼Œæ¨è16GB+
- **ç£ç›˜**: æœ€å°‘50GBå¯ç”¨ç©ºé—´
- **CPU**: æœ€å°‘4æ ¸ï¼Œæ¨è8æ ¸+

#### è½¯ä»¶ä¾èµ–
```bash
# å®‰è£…Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# å®‰è£…Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# éªŒè¯å®‰è£…
docker --version
docker-compose --version
```

### 2. é¡¹ç›®éƒ¨ç½²

#### å…‹éš†é¡¹ç›®
```bash
git clone <repository-url>
cd large-data
```

#### é…ç½®ç¯å¢ƒå˜é‡
```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘é…ç½®
vim .env
```

#### å¯åŠ¨æœåŠ¡
```bash
# ç»™è„šæœ¬æ‰§è¡Œæƒé™
chmod +x *.sh

# å¯åŠ¨æ‰€æœ‰æœåŠ¡
./start.sh

# æˆ–åˆ†æ­¥å¯åŠ¨ï¼ˆæ¨èï¼‰
./start-basic.sh
./start-hdfs-spark.sh
```

### 3. éªŒè¯éƒ¨ç½²

#### æ£€æŸ¥æœåŠ¡çŠ¶æ€
```bash
# æŸ¥çœ‹æ‰€æœ‰å®¹å™¨çŠ¶æ€
docker-compose ps

# æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
./verify-data.sh
```

#### è®¿é—®æœåŠ¡
- Airflow: http://your-server:8080
- Spark: http://your-server:8081
- HDFS: http://your-server:9870

### 4. å®‰å…¨é…ç½®

#### ä¿®æ”¹é»˜è®¤å¯†ç 
```bash
# Airflowç®¡ç†å‘˜å¯†ç 
docker-compose exec airflow-webserver airflow users create \
    --username your-admin \
    --firstname Your \
    --lastname Name \
    --role Admin \
    --email your-email@domain.com \
    --password your-secure-password
```

#### ç½‘ç»œå®‰å…¨
```bash
# é…ç½®é˜²ç«å¢™ï¼ˆUbuntuï¼‰
sudo ufw allow 22/tcp
sudo ufw allow 8080/tcp  # Airflow
sudo ufw allow 8081/tcp  # Spark
sudo ufw allow 9870/tcp  # HDFS
sudo ufw enable
```

### 5. ç›‘æ§å’Œç»´æŠ¤

#### æ—¥å¿—ç®¡ç†
```bash
# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
docker-compose logs -f airflow-scheduler

# æ¸…ç†æ—§æ—¥å¿—
docker system prune -f
```

#### å¤‡ä»½æ•°æ®
```bash
# å¤‡ä»½MySQLæ•°æ®
docker-compose exec mysql mysqldump -u root -prootpass --all-databases > backup.sql

# å¤‡ä»½Airflowé…ç½®
docker-compose exec postgres pg_dump -U airflow airflow > airflow_backup.sql
```

#### æ›´æ–°æœåŠ¡
```bash
# æ‹‰å–æœ€æ–°é•œåƒ
docker-compose pull

# é‡å¯æœåŠ¡
docker-compose down
docker-compose up -d
```

## ğŸ”§ é…ç½®ä¼˜åŒ–

### èµ„æºé…ç½®

#### Sparké…ç½®ä¼˜åŒ–
ç¼–è¾‘ `spark-defaults.conf.txt`:
```
spark.executor.memory            2g
spark.driver.memory              1g
spark.executor.cores             2
spark.sql.adaptive.enabled       true
```

#### Airflowé…ç½®ä¼˜åŒ–
åœ¨ `docker-compose.yaml` ä¸­è°ƒæ•´:
```yaml
environment:
  AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 1
  AIRFLOW__CORE__PARALLELISM: 32
  AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG: 16
```

### å­˜å‚¨é…ç½®

#### HDFSå­˜å‚¨ä¼˜åŒ–
```bash
# å¢åŠ HDFSå‰¯æœ¬æ•°ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
docker-compose exec namenode hdfs dfsadmin -setDefaultReplication 3
```

#### MySQLå­˜å‚¨ä¼˜åŒ–
åœ¨ `docker-compose.yaml` ä¸­æ·»åŠ :
```yaml
mysql:
  command: --innodb-buffer-pool-size=1G --max-connections=200
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. å†…å­˜ä¸è¶³
```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h
docker stats

# è°ƒæ•´æœåŠ¡å†…å­˜é™åˆ¶
# åœ¨docker-compose.yamlä¸­æ·»åŠ :
deploy:
  resources:
    limits:
      memory: 2G
```

#### 2. ç£ç›˜ç©ºé—´ä¸è¶³
```bash
# æ¸…ç†Dockerèµ„æº
docker system prune -a -f

# æ¸…ç†æ—¥å¿—æ–‡ä»¶
sudo find /var/lib/docker -name "*.log" -delete
```

#### 3. ç½‘ç»œè¿æ¥é—®é¢˜
```bash
# é‡å»ºç½‘ç»œ
docker-compose down
docker network prune -f
docker-compose up -d
```

### ç›‘æ§è„šæœ¬

åˆ›å»ºç›‘æ§è„šæœ¬ `monitor.sh`:
```bash
#!/bin/bash
# æ£€æŸ¥å…³é”®æœåŠ¡çŠ¶æ€
services=("airflow-webserver" "spark-master" "namenode" "mysql")
for service in "${services[@]}"; do
    if docker-compose ps $service | grep -q "Up"; then
        echo "âœ… $service: Running"
    else
        echo "âŒ $service: Down"
        # å‘é€å‘Šè­¦é€šçŸ¥
        # curl -X POST "your-webhook-url" -d "Service $service is down"
    fi
done
```

## ğŸ“ˆ æ€§èƒ½è°ƒä¼˜

### 1. JVMè°ƒä¼˜
åœ¨ç›¸å…³æœåŠ¡ä¸­æ·»åŠ JVMå‚æ•°:
```yaml
environment:
  JAVA_OPTS: "-Xmx2g -Xms1g -XX:+UseG1GC"
```

### 2. æ•°æ®åº“è°ƒä¼˜
MySQLé…ç½®ä¼˜åŒ–:
```sql
SET GLOBAL innodb_buffer_pool_size = 1073741824;  -- 1GB
SET GLOBAL max_connections = 200;
SET GLOBAL query_cache_size = 268435456;  -- 256MB
```

### 3. ç½‘ç»œè°ƒä¼˜
```bash
# å¢åŠ ç½‘ç»œç¼“å†²åŒº
echo 'net.core.rmem_max = 16777216' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 16777216' >> /etc/sysctl.conf
sysctl -p
```

## ğŸ”„ CI/CDé›†æˆ

### GitHub Actionsç¤ºä¾‹
åˆ›å»º `.github/workflows/deploy.yml`:
```yaml
name: Deploy Big Data Platform

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Deploy to server
      run: |
        ssh user@server 'cd /path/to/project && git pull && ./start.sh'
```

## ğŸ“ æ”¯æŒ

å¦‚é‡åˆ°éƒ¨ç½²é—®é¢˜ï¼Œè¯·ï¼š
1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
2. è¿è¡Œæ•…éšœæ’é™¤è„šæœ¬
3. æŸ¥çœ‹GitHub Issues
4. è”ç³»æŠ€æœ¯æ”¯æŒ