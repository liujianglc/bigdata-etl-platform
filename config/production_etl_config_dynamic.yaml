# 动态生产环境大数据ETL配置文件
# 支持从环境变量读取配置，适配Docker Compose

# MySQL数据源配置
mysql:
  host: "${MYSQL_HOST:-mysql}"  # Docker容器内访问
  port: ${MYSQL_PORT:-3306}
  username: "${MYSQL_USER:-etl_user}"
  password: "${MYSQL_PASSWORD:-etl_pass}"
  database: "${MYSQL_DATABASE:-wudeli}"
  
  # 连接池配置
  connection_pool:
    max_connections: 10
    connection_timeout: 30
    
  # 增量更新配置
  incremental:
    default_column: "updated_at"
    lookback_days: 1
    batch_size: 10000

# Spark集群配置
spark:
  master: "spark://${SPARK_MASTER_HOST:-spark-master}:${SPARK_MASTER_PORT:-7077}"
  deploy_mode: "client"
  
  # 资源配置
  driver:
    memory: "2g"
    cores: 1
    
  executor:
    memory: "${SPARK_WORKER_MEMORY:-2g}"
    cores: ${SPARK_WORKER_CORES:-2}
    instances: 2
    
  # 优化配置
  conf:
    spark.sql.adaptive.enabled: true
    spark.sql.adaptive.coalescePartitions.enabled: true
    spark.sql.adaptive.skewJoin.enabled: true
    spark.serializer: "org.apache.spark.serializer.KryoSerializer"
    spark.sql.sources.partitionOverwriteMode: "dynamic"

# HDFS存储配置
hdfs:
  namenode: "hdfs://${HDFS_NAMENODE_HOST:-namenode}:${HDFS_NAMENODE_PORT:-9000}"
  
  # 目录结构
  paths:
    base: "/user/bigdata/wudeli"
    raw: "/user/bigdata/wudeli/raw"
    processed: "/user/bigdata/wudeli/processed" 
    reports: "/user/bigdata/wudeli/reports"
    temp: "/user/bigdata/wudeli/temp"
    archive: "/user/bigdata/wudeli/archive"
    
  # 文件格式配置
  storage:
    format: "parquet"
    compression: "snappy"
    partition_strategy: "date"

# Hive数据仓库配置  
hive:
  database: "wudeli_analytics"
  metastore_uri: "thrift://hive-metastore:${HIVE_METASTORE_PORT:-9083}"
  
  # 表配置
  tables:
    main_data: "main_data"
    orders: "orders"
    
  # 分析视图
  views:
    - name: "daily_summary"
      description: "每日业务指标汇总"
    - name: "product_ranking"
      description: "产品销售业绩排行"
    - name: "customer_analysis" 
      description: "客户行为深度分析"
    - name: "monthly_trends"
      description: "月度业务趋势分析"

# ETL作业配置
etl:
  # 调度配置
  schedule:
    cron: "${ETL_SCHEDULE_CRON:-0 2 * * *}"  # 从环境变量读取，默认每天凌晨2点
    timezone: "${ETL_TIMEZONE:-Asia/Shanghai}"
    
  # 重试配置  
  retry:
    attempts: ${ETL_RETRY_ATTEMPTS:-2}
    delay_minutes: ${ETL_RETRY_DELAY:-10}
    exponential_backoff: true
    max_delay_hours: 1
    
  # 数据质量检查
  data_quality:
    enable_validation: true
    null_threshold: ${DATA_QUALITY_NULL_THRESHOLD:-0.05}
    duplicate_check: true
    freshness_hours: ${DATA_QUALITY_FRESHNESS_HOURS:-25}

# 监控和告警配置
monitoring:
  # 日志级别
  log_level: "${LOG_LEVEL:-INFO}"
  
  # 性能指标
  metrics:
    enable_spark_ui: ${ENABLE_METRICS:-true}
    enable_hdfs_metrics: ${ENABLE_METRICS:-true}
    track_execution_time: true
    
  # 告警配置
  alerts:
    email_on_failure: ${ENABLE_ALERTS:-true}
    email_on_retry: false
    slack_webhook: "${SLACK_WEBHOOK:-}"
    
  # 报告配置
  reports:
    generate_html: true
    save_to_hdfs: true
    retention_days: 30

# 安全配置
security:
  # 使用Airflow连接管理密码
  use_airflow_connections: true
  encrypt_passwords: true
  
  # Kerberos认证（如果启用）
  kerberos:
    enabled: false
    principal: ""
    keytab: ""

# 服务端口配置（用于健康检查和监控）
services:
  airflow:
    webserver_port: ${AIRFLOW_WEBSERVER_PORT:-8080}
    scheduler_port: ${AIRFLOW_SCHEDULER_PORT:-8974}
  spark:
    master_ui_port: ${SPARK_UI_PORT:-8081}
    master_port: ${SPARK_MASTER_PORT:-7077}
  hdfs:
    namenode_ui_port: ${HDFS_UI_PORT:-9870}
    namenode_port: ${HDFS_NAMENODE_PORT:-9000}
  hive:
    metastore_port: ${HIVE_METASTORE_PORT:-9083}