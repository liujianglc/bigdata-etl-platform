from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.bash import BashOperator

import pandas as pd
from airflow.models import Variable
import logging

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True,
    'email_on_retry': False,
}

def call_deepseek_api(prompt, system_message, analysis_type="é€šç”¨", max_tokens=2000):
    """
    ç»Ÿä¸€çš„DeepSeek APIè°ƒç”¨å‡½æ•°ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
    """
    import requests
    import time
    import logging
    
    try:
        deepseek_api_key = Variable.get("DEEPSEEK_API_KEY", default_var="your_deepseek_api_key")
        
        # ä»Airflow Variablesè·å–é…ç½®ï¼Œæä¾›é»˜è®¤å€¼
        max_retries = int(Variable.get("DEEPSEEK_MAX_RETRIES", default_var="3"))
        retry_delay = int(Variable.get("DEEPSEEK_RETRY_DELAY", default_var="5"))
        timeout = int(Variable.get("DEEPSEEK_TIMEOUT", default_var="60"))
        
        for attempt in range(max_retries):
            try:
                logging.info(f"{analysis_type}DeepSeek APIè°ƒç”¨å°è¯• {attempt + 1}/{max_retries}")
                
                response = requests.post(
                    "https://api.deepseek.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {deepseek_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "deepseek-chat",
                        "messages": [
                            {"role": "system", "content": system_message},
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.3,
                        "max_tokens": max_tokens
                    },
                    timeout=timeout
                )
                
                if response.status_code == 200:
                    analysis_result = response.json()['choices'][0]['message']['content']
                    logging.info(f"âœ… {analysis_type}DeepSeekåˆ†æå®Œæˆ")
                    return analysis_result
                else:
                    logging.warning(f"{analysis_type}DeepSeek APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        continue
                    else:
                        return f"{analysis_type}LLMåˆ†ææœåŠ¡è¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}"
                        
            except requests.exceptions.Timeout as e:
                logging.warning(f"{analysis_type}DeepSeek APIè¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    return f"{analysis_type}LLMåˆ†ææœåŠ¡è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è”ç³»ç®¡ç†å‘˜ã€‚"
                    
            except requests.exceptions.ConnectionError as e:
                logging.warning(f"{analysis_type}DeepSeek APIè¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    return f"{analysis_type}LLMåˆ†ææœåŠ¡è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚"
                    
    except Exception as e:
        logging.error(f"{analysis_type}LLMåˆ†æå¤±è´¥: {e}")
        return f"{analysis_type}LLMåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"

def analyze_daily_kpi_with_llm(**context):
    """ä½¿ç”¨LLMåˆ†ææ—¥KPIæ•°æ®"""
    from pyspark.sql import SparkSession
    import json
    import logging
    from datetime import datetime, timedelta
    
    spark = None
    try:
        # åˆ›å»ºSparkä¼šè¯
        spark = SparkSession.builder \
            .appName("LLM Daily KPI Analysis") \
            .master("local[1]") \
            .config("spark.sql.catalogImplementation", "hive") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
            .config("spark.hadoop.hive.metastore.uris", "thrift://hive-metastore:9083") \
            .config("spark.driver.memory", "1g") \
            .config("spark.executor.memory", "1g") \
            .config("spark.ui.enabled", "false") \
            .enableHiveSupport() \
            .getOrCreate()
        
        spark.sql("USE dws_db")
        
        # è·å–æœ€è¿‘7å¤©çš„æ—¥KPIæ•°æ®
        batch_date = context['ds']
        start_date = (datetime.strptime(batch_date, '%Y-%m-%d') - timedelta(days=6)).strftime('%Y-%m-%d')
        
        daily_kpi_query = f"""
        SELECT 
            order_date,
            total_orders,
            total_amount,
            avg_order_value,
            completion_rate,
            cancellation_rate,
            delay_rate,
            performance_grade
        FROM dws_daily_kpi
        WHERE order_date >= '{start_date}' AND order_date <= '{batch_date}'
        ORDER BY order_date DESC
        """
        
        daily_kpi_df = spark.sql(daily_kpi_query)
        daily_kpi_data = daily_kpi_df.collect()
        
        if not daily_kpi_data:
            logging.warning("æ²¡æœ‰æ‰¾åˆ°æ—¥KPIæ•°æ®")
            return "no_data"
        
        # å‡†å¤‡æ•°æ®ç»™LLMåˆ†æ
        kpi_summary = []
        for row in daily_kpi_data:
            kpi_summary.append({
                'date': str(row['order_date']),
                'orders': int(row['total_orders']) if row['total_orders'] else 0,
                'amount': float(row['total_amount']) if row['total_amount'] else 0.0,
                'avg_order_value': float(row['avg_order_value']) if row['avg_order_value'] else 0.0,
                'completion_rate': float(row['completion_rate']) if row['completion_rate'] else 0.0,
                'cancellation_rate': float(row['cancellation_rate']) if row['cancellation_rate'] else 0.0,
                'delay_rate': float(row['delay_rate']) if row['delay_rate'] else 0.0,
                'performance_grade': str(row['performance_grade']) if row['performance_grade'] else 'Unknown'
            })
        
        # ä½¿ç”¨agno + deepseekè¿›è¡Œåˆ†æ
        analysis_prompt = f"""
        ä½œä¸ºä¸€åèµ„æ·±çš„æ•°æ®åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹æœ€è¿‘7å¤©çš„è®¢å•KPIæ•°æ®ï¼š

        {json.dumps(kpi_summary, indent=2, ensure_ascii=False)}

        è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼š
        1. è®¢å•é‡è¶‹åŠ¿åˆ†æï¼ˆå¢é•¿/ä¸‹é™è¶‹åŠ¿ï¼Œå¼‚å¸¸æ³¢åŠ¨ï¼‰
        2. è®¢å•é‡‘é¢å˜åŒ–åˆ†æï¼ˆæ”¶å…¥è¶‹åŠ¿ï¼Œå¹³å‡è®¢å•ä»·å€¼å˜åŒ–ï¼‰
        3. è¿è¥æ•ˆç‡åˆ†æï¼ˆå®Œæˆç‡ã€å–æ¶ˆç‡ã€å»¶è¿Ÿç‡çš„è¡¨ç°ï¼‰
        4. æ€§èƒ½ç­‰çº§åˆ†å¸ƒå’Œå˜åŒ–
        5. å…³é”®é£é™©ç‚¹è¯†åˆ«
        6. ä¸šåŠ¡æ”¹è¿›å»ºè®®

        è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæä¾›å…·ä½“çš„æ•°æ®æ´å¯Ÿå’Œå¯æ‰§è¡Œçš„å»ºè®®ã€‚
        """
        
        # ä½¿ç”¨ç»Ÿä¸€çš„APIè°ƒç”¨å‡½æ•°
        analysis_result = call_deepseek_api(
            prompt=analysis_prompt,
            system_message="ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ç”µå•†è®¢å•æ•°æ®åˆ†æã€‚",
            analysis_type="æ—¥KPI",
            max_tokens=2000
        )
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_data = {
            'analysis_date': batch_date,
            'data_period': f"{start_date} to {batch_date}",
            'kpi_data': kpi_summary,
            'llm_analysis': analysis_result,
            'analysis_type': 'daily_kpi'
        }
        
        context['task_instance'].xcom_push(key='daily_kpi_analysis', value=analysis_data)
        
        logging.info("âœ… æ—¥KPI LLMåˆ†æå®Œæˆ")
        return analysis_data
        
    except Exception as e:
        logging.error(f"æ—¥KPIåˆ†æå¤±è´¥: {e}")
        raise
    finally:
        if spark:
            spark.stop()

def analyze_customer_segments_with_llm(**context):
    """ä½¿ç”¨LLMåˆ†æå®¢æˆ·åˆ†æ®µæ•°æ®"""
    from pyspark.sql import SparkSession
    import json
    import logging
    
    spark = None
    try:
        spark = SparkSession.builder \
            .appName("LLM Customer Segments Analysis") \
            .master("local[1]") \
            .config("spark.sql.catalogImplementation", "hive") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
            .config("spark.hadoop.hive.metastore.uris", "thrift://hive-metastore:9083") \
            .config("spark.driver.memory", "1g") \
            .config("spark.executor.memory", "1g") \
            .config("spark.ui.enabled", "false") \
            .enableHiveSupport() \
            .getOrCreate()
        
        spark.sql("USE dws_db")
        
        # è·å–å®¢æˆ·ä»·å€¼åˆ†æ®µæ•°æ®
        customer_segments_query = """
        SELECT 
            customer_segment,
            customer_count,
            segment_revenue,
            avg_customer_value,
            avg_orders_per_customer,
            avg_order_value,
            active_customers,
            ROUND(active_customers * 100.0 / customer_count, 2) as active_rate
        FROM dws_customer_value_segments
        ORDER BY 
            CASE customer_segment 
                WHEN 'Platinum' THEN 1
                WHEN 'Gold' THEN 2
                WHEN 'Silver' THEN 3
                WHEN 'Bronze' THEN 4
            END
        """
        
        segments_df = spark.sql(customer_segments_query)
        segments_data = segments_df.collect()
        
        # è·å–å®¢æˆ·çŠ¶æ€åˆ†ææ•°æ®
        customer_status_query = """
        SELECT 
            customer_status,
            customer_count,
            status_revenue,
            avg_days_since_last_order,
            avg_monthly_frequency
        FROM dws_customer_status_analysis
        ORDER BY customer_count DESC
        """
        
        status_df = spark.sql(customer_status_query)
        status_data = status_df.collect()
        
        if not segments_data and not status_data:
            logging.warning("æ²¡æœ‰æ‰¾åˆ°å®¢æˆ·åˆ†ææ•°æ®")
            return "no_data"
        
        # å‡†å¤‡æ•°æ®ç»™LLMåˆ†æ
        segments_summary = []
        for row in segments_data:
            segments_summary.append({
                'segment': str(row['customer_segment']),
                'customer_count': int(row['customer_count']) if row['customer_count'] else 0,
                'segment_revenue': float(row['segment_revenue']) if row['segment_revenue'] else 0.0,
                'avg_customer_value': float(row['avg_customer_value']) if row['avg_customer_value'] else 0.0,
                'avg_orders_per_customer': float(row['avg_orders_per_customer']) if row['avg_orders_per_customer'] else 0.0,
                'avg_order_value': float(row['avg_order_value']) if row['avg_order_value'] else 0.0,
                'active_customers': int(row['active_customers']) if row['active_customers'] else 0,
                'active_rate': float(row['active_rate']) if row['active_rate'] else 0.0
            })
        
        status_summary = []
        for row in status_data:
            status_summary.append({
                'status': str(row['customer_status']),
                'customer_count': int(row['customer_count']) if row['customer_count'] else 0,
                'status_revenue': float(row['status_revenue']) if row['status_revenue'] else 0.0,
                'avg_days_since_last_order': float(row['avg_days_since_last_order']) if row['avg_days_since_last_order'] else 0.0,
                'avg_monthly_frequency': float(row['avg_monthly_frequency']) if row['avg_monthly_frequency'] else 0.0
            })
        
        # ä½¿ç”¨agno + deepseekè¿›è¡Œåˆ†æ
        analysis_prompt = f"""
        ä½œä¸ºä¸€åå®¢æˆ·å…³ç³»ç®¡ç†ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹å®¢æˆ·åˆ†æ®µå’ŒçŠ¶æ€æ•°æ®ï¼š

        å®¢æˆ·ä»·å€¼åˆ†æ®µæ•°æ®ï¼š
        {json.dumps(segments_summary, indent=2, ensure_ascii=False)}

        å®¢æˆ·çŠ¶æ€åˆ†ææ•°æ®ï¼š
        {json.dumps(status_summary, indent=2, ensure_ascii=False)}

        è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼š
        1. å®¢æˆ·ä»·å€¼åˆ†æ®µåˆ†æï¼ˆå„åˆ†æ®µçš„è´¡çŒ®åº¦ã€ç‰¹å¾ã€ä»·å€¼å¯†åº¦ï¼‰
        2. å®¢æˆ·æ´»è·ƒåº¦åˆ†æï¼ˆæ´»è·ƒç‡ã€æµå¤±é£é™©ã€ç”Ÿå‘½å‘¨æœŸçŠ¶æ€ï¼‰
        3. æ”¶å…¥ç»“æ„åˆ†æï¼ˆå„åˆ†æ®µå’ŒçŠ¶æ€çš„æ”¶å…¥è´¡çŒ®ï¼‰
        4. å®¢æˆ·è¡Œä¸ºæ¨¡å¼è¯†åˆ«ï¼ˆè®¢å•é¢‘æ¬¡ã€æ¶ˆè´¹ä¹ æƒ¯ï¼‰
        5. å®¢æˆ·æµå¤±é£é™©è¯„ä¼°
        6. å®¢æˆ·ä»·å€¼æå‡ç­–ç•¥å»ºè®®
        7. ç²¾å‡†è¥é”€å»ºè®®

        è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæä¾›å…·ä½“çš„å®¢æˆ·æ´å¯Ÿå’Œå¯æ‰§è¡Œçš„CRMç­–ç•¥ã€‚
        """
        
        # ä½¿ç”¨ç»Ÿä¸€çš„APIè°ƒç”¨å‡½æ•°
        analysis_result = call_deepseek_api(
            prompt=analysis_prompt,
            system_message="ä½ æ˜¯ä¸€åä¸“ä¸šçš„å®¢æˆ·å…³ç³»ç®¡ç†ä¸“å®¶ï¼Œæ“…é•¿å®¢æˆ·åˆ†æå’ŒCRMç­–ç•¥åˆ¶å®šã€‚",
            analysis_type="å®¢æˆ·åˆ†æ®µ",
            max_tokens=2500
        )
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_data = {
            'analysis_date': context['ds'],
            'segments_data': segments_summary,
            'status_data': status_summary,
            'llm_analysis': analysis_result,
            'analysis_type': 'customer_segments'
        }
        
        context['task_instance'].xcom_push(key='customer_segments_analysis', value=analysis_data)
        
        logging.info("âœ… å®¢æˆ·åˆ†æ®µLLMåˆ†æå®Œæˆ")
        return analysis_data
        
    except Exception as e:
        logging.error(f"å®¢æˆ·åˆ†æ®µåˆ†æå¤±è´¥: {e}")
        raise
    finally:
        if spark:
            spark.stop()

def analyze_monthly_trends_with_llm(**context):
    """ä½¿ç”¨LLMåˆ†ææœˆåº¦è¶‹åŠ¿æ•°æ®"""
    from pyspark.sql import SparkSession
    import json
    import logging
    from datetime import datetime, timedelta
    
    spark = None
    try:
        spark = SparkSession.builder \
            .appName("LLM Monthly Trends Analysis") \
            .master("local[1]") \
            .config("spark.sql.catalogImplementation", "hive") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
            .config("spark.hadoop.hive.metastore.uris", "thrift://hive-metastore:9083") \
            .config("spark.driver.memory", "1g") \
            .config("spark.executor.memory", "1g") \
            .config("spark.ui.enabled", "false") \
            .enableHiveSupport() \
            .getOrCreate()
        
        spark.sql("USE dws_db")
        
        # è·å–æœ€è¿‘6ä¸ªæœˆçš„æœˆåº¦è¶‹åŠ¿æ•°æ®
        batch_date = context['ds']
        current_date = datetime.strptime(batch_date, '%Y-%m-%d')
        
        monthly_trends_query = """
        SELECT 
            year_month,
            total_orders,
            total_amount,
            avg_order_value,
            vip_ratio,
            large_order_ratio,
            completion_rate,
            prev_month_orders,
            prev_month_amount,
            CASE 
                WHEN prev_month_orders > 0 THEN 
                    ROUND((total_orders - prev_month_orders) * 100.0 / prev_month_orders, 2)
                ELSE NULL 
            END as orders_growth_rate,
            CASE 
                WHEN prev_month_amount > 0 THEN 
                    ROUND((total_amount - prev_month_amount) * 100.0 / prev_month_amount, 2)
                ELSE NULL 
            END as amount_growth_rate
        FROM dws_orders_monthly_trend
        ORDER BY year_month DESC
        LIMIT 6
        """
        
        trends_df = spark.sql(monthly_trends_query)
        trends_data = trends_df.collect()
        
        # è·å–æœˆåº¦ä¸šåŠ¡æŒ‡æ ‡æ•°æ®
        business_metrics_query = """
        SELECT 
            year_month,
            total_orders,
            total_amount,
            vip_orders,
            large_orders,
            vip_percentage,
            large_order_percentage,
            completion_percentage,
            avg_processing_days
        FROM dws_monthly_business_metrics
        ORDER BY year_month DESC
        LIMIT 6
        """
        
        metrics_df = spark.sql(business_metrics_query)
        metrics_data = metrics_df.collect()
        
        if not trends_data and not metrics_data:
            logging.warning("æ²¡æœ‰æ‰¾åˆ°æœˆåº¦è¶‹åŠ¿æ•°æ®")
            return "no_data"
        
        # å‡†å¤‡æ•°æ®ç»™LLMåˆ†æ
        trends_summary = []
        for row in trends_data:
            trends_summary.append({
                'month': str(row['year_month']),
                'total_orders': int(row['total_orders']) if row['total_orders'] else 0,
                'total_amount': float(row['total_amount']) if row['total_amount'] else 0.0,
                'avg_order_value': float(row['avg_order_value']) if row['avg_order_value'] else 0.0,
                'vip_ratio': float(row['vip_ratio']) if row['vip_ratio'] else 0.0,
                'large_order_ratio': float(row['large_order_ratio']) if row['large_order_ratio'] else 0.0,
                'completion_rate': float(row['completion_rate']) if row['completion_rate'] else 0.0,
                'orders_growth_rate': float(row['orders_growth_rate']) if row['orders_growth_rate'] else None,
                'amount_growth_rate': float(row['amount_growth_rate']) if row['amount_growth_rate'] else None
            })
        
        metrics_summary = []
        for row in metrics_data:
            metrics_summary.append({
                'month': str(row['year_month']),
                'vip_orders': int(row['vip_orders']) if row['vip_orders'] else 0,
                'large_orders': int(row['large_orders']) if row['large_orders'] else 0,
                'vip_percentage': float(row['vip_percentage']) if row['vip_percentage'] else 0.0,
                'large_order_percentage': float(row['large_order_percentage']) if row['large_order_percentage'] else 0.0,
                'completion_percentage': float(row['completion_percentage']) if row['completion_percentage'] else 0.0,
                'avg_processing_days': float(row['avg_processing_days']) if row['avg_processing_days'] else 0.0
            })
        
        # ä½¿ç”¨agno + deepseekè¿›è¡Œåˆ†æ
        analysis_prompt = f"""
        ä½œä¸ºä¸€åä¸šåŠ¡åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹æœ€è¿‘6ä¸ªæœˆçš„è®¢å•è¶‹åŠ¿æ•°æ®ï¼š

        æœˆåº¦è¶‹åŠ¿æ•°æ®ï¼š
        {json.dumps(trends_summary, indent=2, ensure_ascii=False)}

        æœˆåº¦ä¸šåŠ¡æŒ‡æ ‡æ•°æ®ï¼š
        {json.dumps(metrics_summary, indent=2, ensure_ascii=False)}

        è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ·±å…¥åˆ†æï¼š
        1. è®¢å•é‡å’Œæ”¶å…¥è¶‹åŠ¿åˆ†æï¼ˆå¢é•¿æ¨¡å¼ã€å­£èŠ‚æ€§ç‰¹å¾ã€å¼‚å¸¸æ³¢åŠ¨ï¼‰
        2. å¹³å‡è®¢å•ä»·å€¼å˜åŒ–åˆ†æï¼ˆä»·å€¼æå‡/ä¸‹é™åŸå› ï¼‰
        3. VIPå®¢æˆ·å’Œå¤§è®¢å•å æ¯”è¶‹åŠ¿ï¼ˆé«˜ä»·å€¼å®¢æˆ·å‘å±•æƒ…å†µï¼‰
        4. è¿è¥æ•ˆç‡è¶‹åŠ¿ï¼ˆå®Œæˆç‡ã€å¤„ç†æ—¶é—´å˜åŒ–ï¼‰
        5. å¢é•¿ç‡åˆ†æï¼ˆç¯æ¯”å¢é•¿çš„ç¨³å®šæ€§å’Œå¯æŒç»­æ€§ï¼‰
        6. ä¸šåŠ¡å¥åº·åº¦è¯„ä¼°
        7. æœªæ¥è¶‹åŠ¿é¢„æµ‹å’Œæˆ˜ç•¥å»ºè®®

        è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæä¾›å…·ä½“çš„è¶‹åŠ¿æ´å¯Ÿå’Œæˆ˜ç•¥å»ºè®®ã€‚
        """
        
        # ä½¿ç”¨ç»Ÿä¸€çš„APIè°ƒç”¨å‡½æ•°
        analysis_result = call_deepseek_api(
            prompt=analysis_prompt,
            system_message="ä½ æ˜¯ä¸€åä¸“ä¸šçš„ä¸šåŠ¡åˆ†æä¸“å®¶ï¼Œæ“…é•¿è¶‹åŠ¿åˆ†æå’Œæˆ˜ç•¥è§„åˆ’ã€‚",
            analysis_type="æœˆåº¦è¶‹åŠ¿",
            max_tokens=2500
        )
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_data = {
            'analysis_date': context['ds'],
            'trends_data': trends_summary,
            'metrics_data': metrics_summary,
            'llm_analysis': analysis_result,
            'analysis_type': 'monthly_trends'
        }
        
        context['task_instance'].xcom_push(key='monthly_trends_analysis', value=analysis_data)
        
        logging.info("âœ… æœˆåº¦è¶‹åŠ¿LLMåˆ†æå®Œæˆ")
        return analysis_data
        
    except Exception as e:
        logging.error(f"æœˆåº¦è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
        raise
    finally:
        if spark:
            spark.stop()

def generate_comprehensive_report(**context):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    import logging
    from datetime import datetime
    
    try:
        # è·å–æ‰€æœ‰åˆ†æç»“æœ
        daily_kpi_analysis = context['task_instance'].xcom_pull(task_ids='analyze_daily_kpi_with_llm', key='daily_kpi_analysis')
        customer_segments_analysis = context['task_instance'].xcom_pull(task_ids='analyze_customer_segments_with_llm', key='customer_segments_analysis')
        monthly_trends_analysis = context['task_instance'].xcom_pull(task_ids='analyze_monthly_trends_with_llm', key='monthly_trends_analysis')
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        report_html = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>è®¢å•æ•°æ®æ™ºèƒ½åˆ†ææŠ¥å‘Š - {context['ds']}</title>
            <style>
                body {{ font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
                .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                .header {{ text-align: center; margin-bottom: 30px; padding-bottom: 20px; border-bottom: 2px solid #e0e0e0; }}
                .header h1 {{ color: #2c3e50; margin-bottom: 10px; }}
                .header p {{ color: #7f8c8d; font-size: 16px; }}
                .section {{ margin-bottom: 40px; }}
                .section h2 {{ color: #34495e; border-left: 4px solid #3498db; padding-left: 15px; margin-bottom: 20px; }}
                .analysis-content {{ background-color: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid #17a2b8; }}
                .data-summary {{ background-color: #e8f4f8; padding: 15px; border-radius: 5px; margin-bottom: 15px; }}
                .footer {{ text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid #e0e0e0; color: #7f8c8d; }}
                .highlight {{ background-color: #fff3cd; padding: 10px; border-radius: 5px; border-left: 4px solid #ffc107; margin: 10px 0; }}
                pre {{ background-color: #f4f4f4; padding: 15px; border-radius: 5px; overflow-x: auto; white-space: pre-wrap; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ğŸ¤– è®¢å•æ•°æ®æ™ºèƒ½åˆ†ææŠ¥å‘Š</h1>
                    <p>åŸºäºDeepSeekå¤§æ¨¡å‹çš„æ·±åº¦æ•°æ®æ´å¯Ÿ | åˆ†ææ—¥æœŸ: {context['ds']}</p>
                </div>
        """
        
        # æ·»åŠ æ—¥KPIåˆ†æ
        if daily_kpi_analysis and daily_kpi_analysis != "no_data":
            report_html += f"""
                <div class="section">
                    <h2>ğŸ“Š æ—¥åº¦KPIåˆ†æ</h2>
                    <div class="data-summary">
                        <strong>åˆ†æå‘¨æœŸ:</strong> {daily_kpi_analysis.get('data_period', 'N/A')}<br>
                        <strong>æ•°æ®ç‚¹æ•°:</strong> {len(daily_kpi_analysis.get('kpi_data', []))} å¤©
                    </div>
                    <div class="analysis-content">
                        <pre>{daily_kpi_analysis.get('llm_analysis', 'åˆ†æç»“æœä¸å¯ç”¨')}</pre>
                    </div>
                </div>
            """
        
        # æ·»åŠ å®¢æˆ·åˆ†æ®µåˆ†æ
        if customer_segments_analysis and customer_segments_analysis != "no_data":
            report_html += f"""
                <div class="section">
                    <h2>ğŸ‘¥ å®¢æˆ·åˆ†æ®µåˆ†æ</h2>
                    <div class="data-summary">
                        <strong>åˆ†ææ—¥æœŸ:</strong> {customer_segments_analysis.get('analysis_date', 'N/A')}<br>
                        <strong>å®¢æˆ·åˆ†æ®µæ•°:</strong> {len(customer_segments_analysis.get('segments_data', []))}<br>
                        <strong>çŠ¶æ€åˆ†ç±»æ•°:</strong> {len(customer_segments_analysis.get('status_data', []))}
                    </div>
                    <div class="analysis-content">
                        <pre>{customer_segments_analysis.get('llm_analysis', 'åˆ†æç»“æœä¸å¯ç”¨')}</pre>
                    </div>
                </div>
            """
        
        # æ·»åŠ æœˆåº¦è¶‹åŠ¿åˆ†æ
        if monthly_trends_analysis and monthly_trends_analysis != "no_data":
            report_html += f"""
                <div class="section">
                    <h2>ğŸ“ˆ æœˆåº¦è¶‹åŠ¿åˆ†æ</h2>
                    <div class="data-summary">
                        <strong>åˆ†ææ—¥æœŸ:</strong> {monthly_trends_analysis.get('analysis_date', 'N/A')}<br>
                        <strong>è¶‹åŠ¿æ•°æ®æœˆæ•°:</strong> {len(monthly_trends_analysis.get('trends_data', []))}<br>
                        <strong>ä¸šåŠ¡æŒ‡æ ‡æœˆæ•°:</strong> {len(monthly_trends_analysis.get('metrics_data', []))}
                    </div>
                    <div class="analysis-content">
                        <pre>{monthly_trends_analysis.get('llm_analysis', 'åˆ†æç»“æœä¸å¯ç”¨')}</pre>
                    </div>
                </div>
            """
        
        # æ·»åŠ æŠ¥å‘Šå°¾éƒ¨
        report_html += f"""
                <div class="highlight">
                    <strong>ğŸ’¡ æŠ¥å‘Šè¯´æ˜:</strong><br>
                    â€¢ æœ¬æŠ¥å‘ŠåŸºäºDWSå±‚è®¢å•åˆ†ææ•°æ®ï¼Œä½¿ç”¨DeepSeekå¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ†æ<br>
                    â€¢ åˆ†æç»“æœä»…ä¾›å‚è€ƒï¼Œå…·ä½“å†³ç­–è¯·ç»“åˆä¸šåŠ¡å®é™…æƒ…å†µ<br>
                    â€¢ å¦‚æœ‰ç–‘é—®ï¼Œè¯·è”ç³»æ•°æ®å›¢é˜Ÿè¿›è¡Œè¯¦ç»†è§£è¯»
                </div>
                
                <div class="footer">
                    <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | æ•°æ®å›¢é˜Ÿå‡ºå“</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # ä¿å­˜æŠ¥å‘Š
        context['task_instance'].xcom_push(key='comprehensive_report', value=report_html)
        
        logging.info("âœ… ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report_html
        
    except Exception as e:
        logging.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        raise

# åˆ›å»ºDAG
with DAG(
    'llm_orders_analytics',
    default_args=default_args,
    schedule_interval='0 6 * * *',  # æ¯æ—¥æ—©ä¸Š6ç‚¹æ‰§è¡Œï¼ˆåœ¨dws_orders_analyticsä¹‹åï¼‰
    catchup=False,
    max_active_runs=1,
    tags=['llm', 'analytics', 'orders', 'deepseek', 'ai-insights'],
    description='ä½¿ç”¨DeepSeekå¤§æ¨¡å‹åˆ†æDWSå±‚è®¢å•æ•°æ®ï¼Œç”Ÿæˆæ™ºèƒ½æ´å¯ŸæŠ¥å‘Š',
) as dag:

    # å¼€å§‹ä»»åŠ¡
    start_task = DummyOperator(
        task_id='start_llm_analytics',
        doc_md="""
        ## LLMè®¢å•æ•°æ®æ™ºèƒ½åˆ†ææµç¨‹
        
        ä½¿ç”¨DeepSeekå¤§æ¨¡å‹å¯¹DWSå±‚è®¢å•åˆ†ææ•°æ®è¿›è¡Œæ·±åº¦æ´å¯Ÿï¼š
        1. åˆ†ææ—¥åº¦KPIè¶‹åŠ¿å’Œå¼‚å¸¸
        2. åˆ†æå®¢æˆ·åˆ†æ®µå’Œä»·å€¼åˆ†å¸ƒ
        3. åˆ†ææœˆåº¦ä¸šåŠ¡è¶‹åŠ¿
        4. ç”Ÿæˆç»¼åˆæ™ºèƒ½æŠ¥å‘Š
        5. å‘é€é‚®ä»¶é€šçŸ¥ç›¸å…³äººå‘˜
        """
    )

    # æ£€æŸ¥DWSä¾èµ–
    def check_dws_dependencies(**context):
        """æ£€æŸ¥DWSå±‚æ•°æ®æ˜¯å¦å°±ç»ª"""
        from pyspark.sql import SparkSession
        import logging
        
        spark = None
        try:
            spark = SparkSession.builder \
                .appName("Check DWS Dependencies") \
                .master("local[1]") \
                .config("spark.sql.catalogImplementation", "hive") \
                .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
                .config("spark.hadoop.hive.metastore.uris", "thrift://hive-metastore:9083") \
                .config("spark.driver.memory", "1g") \
                .config("spark.executor.memory", "1g") \
                .config("spark.ui.enabled", "false") \
                .enableHiveSupport() \
                .getOrCreate()
            
            # æ£€æŸ¥DWSæ•°æ®åº“å’Œè§†å›¾
            spark.sql("USE dws_db")
            
            required_views = [
                'dws_daily_kpi',
                'dws_customer_value_segments', 
                'dws_customer_status_analysis',
                'dws_orders_monthly_trend',
                'dws_monthly_business_metrics'
            ]
            
            views = spark.sql("SHOW VIEWS").collect()
            available_views = [row[1] for row in views]
            
            missing_views = []
            for view in required_views:
                if view not in available_views:
                    missing_views.append(view)
            
            if missing_views:
                logging.error(f"âŒ ç¼ºå°‘å¿…è¦çš„DWSè§†å›¾: {missing_views}")
                raise Exception(f"è¯·å…ˆè¿è¡Œ dws_orders_analytics åˆ›å»ºå¿…è¦çš„åˆ†æè§†å›¾")
            
            logging.info("âœ… DWSä¾èµ–æ£€æŸ¥é€šè¿‡")
            return "dws_dependencies_ready"
            
        except Exception as e:
            logging.error(f"DWSä¾èµ–æ£€æŸ¥å¤±è´¥: {e}")
            raise
        finally:
            if spark:
                spark.stop()

    check_dependencies_task = PythonOperator(
        task_id='check_dws_dependencies',
        python_callable=check_dws_dependencies
    )

    # LLMåˆ†æä»»åŠ¡
    daily_kpi_analysis_task = PythonOperator(
        task_id='analyze_daily_kpi_with_llm',
        python_callable=analyze_daily_kpi_with_llm,
        doc_md="ä½¿ç”¨DeepSeekåˆ†ææœ€è¿‘7å¤©çš„æ—¥åº¦KPIæ•°æ®"
    )

    customer_segments_analysis_task = PythonOperator(
        task_id='analyze_customer_segments_with_llm',
        python_callable=analyze_customer_segments_with_llm,
        doc_md="ä½¿ç”¨DeepSeekåˆ†æå®¢æˆ·åˆ†æ®µå’ŒçŠ¶æ€æ•°æ®"
    )

    monthly_trends_analysis_task = PythonOperator(
        task_id='analyze_monthly_trends_with_llm',
        python_callable=analyze_monthly_trends_with_llm,
        doc_md="ä½¿ç”¨DeepSeekåˆ†ææœ€è¿‘6ä¸ªæœˆçš„ä¸šåŠ¡è¶‹åŠ¿"
    )

    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    generate_report_task = PythonOperator(
        task_id='generate_comprehensive_report',
        python_callable=generate_comprehensive_report,
        doc_md="æ•´åˆæ‰€æœ‰LLMåˆ†æç»“æœï¼Œç”Ÿæˆç»¼åˆæ™ºèƒ½æŠ¥å‘Š"
    )

    # å‘é€é‚®ä»¶æŠ¥å‘Š
    def get_email_content(**context):
        """è·å–é‚®ä»¶å†…å®¹"""
        report_html = context['task_instance'].xcom_pull(task_ids='generate_comprehensive_report', key='comprehensive_report')
        return report_html or "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚"

    def get_email_config(**context):
        """ä»Airflow Variablesè·å–é‚®ä»¶é…ç½®"""
        from airflow.models import Variable
        import json
        
        # è·å–é‚®ä»¶æ”¶ä»¶äººåˆ—è¡¨
        try:
            recipients_str = Variable.get("EMAIL_RECIPIENTS", default_var='["liujianglc@163.com"]')
            recipients = json.loads(recipients_str)
        except (json.JSONDecodeError, Exception):
            recipients = ['liujianglc@163.com']
        
        # è·å–é‚®ä»¶ä¸»é¢˜
        subject = Variable.get("EMAIL_SUBJECT", default_var="ğŸ“Š è®¢å•æ•°æ®æ™ºèƒ½åˆ†ææŠ¥å‘Š - {{ ds }}")
        
        return recipients, subject

    def send_email_with_config(**context):
        """ä½¿ç”¨é…ç½®å‘é€é‚®ä»¶"""
        from airflow.models import Variable
        import smtplib
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        from email.utils import formatdate
        
        # è·å–é‚®ä»¶é…ç½®
        recipients, subject = get_email_config(**context)
        
        # è·å–æŠ¥å‘Šå†…å®¹
        report_content = context['task_instance'].xcom_pull(
            task_ids='generate_comprehensive_report', 
            key='comprehensive_report'
        ) or "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚"
        
        # è·å–SMTPé…ç½®
        smtp_host = Variable.get("SMTP_HOST", default_var="localhost")
        smtp_port = int(Variable.get("SMTP_PORT", default_var="25"))
        smtp_user = Variable.get("SMTP_USER", default_var="")
        smtp_password = Variable.get("SMTP_PASSWORD", default_var="")
        smtp_use_tls = Variable.get("SMTP_USE_TLS", default_var="False").lower() == "true"
        sender = Variable.get("EMAIL_SENDER", default_var="airflow@localhost")
        
        # åˆ›å»ºé‚®ä»¶
        msg = MIMEMultipart()
        msg['From'] = sender
        msg['To'] = ", ".join(recipients) if isinstance(recipients, list) else recipients
        msg['Date'] = formatdate(localtime=True)
        msg['Subject'] = subject
        
        # æ·»åŠ HTMLå†…å®¹
        msg.attach(MIMEText(report_content, 'html', 'utf-8'))
        
        try:
            # å‘é€é‚®ä»¶
            server = smtplib.SMTP(smtp_host, smtp_port)
            if smtp_use_tls:
                server.starttls()
            if smtp_user and smtp_password:
                server.login(smtp_user, smtp_password)
            
            server.sendmail(sender, recipients, msg.as_string())
            server.quit()
            return "âœ… é‚®ä»¶å‘é€æˆåŠŸ"
        except Exception as e:
            raise Exception(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")

    # ä½¿ç”¨PythonOperatoræ›¿ä»£EmailOperator
    send_email_task = PythonOperator(
        task_id='send_analysis_report',
        python_callable=send_email_with_config,
        doc_md="å‘é€åŒ…å«LLMåˆ†ææ´å¯Ÿçš„ç»¼åˆæŠ¥å‘Šé‚®ä»¶"
    )

    # ç»“æŸä»»åŠ¡
    end_task = DummyOperator(
        task_id='end_llm_analytics',
        doc_md="LLMè®¢å•æ•°æ®æ™ºèƒ½åˆ†ææµç¨‹å®Œæˆ"
    )

    # å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»
    start_task >> check_dependencies_task
    
    # ä¸‰ä¸ªLLMåˆ†æä»»åŠ¡å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
    check_dependencies_task >> [
        daily_kpi_analysis_task,
        customer_segments_analysis_task, 
        monthly_trends_analysis_task
    ]
    
    # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆåç”ŸæˆæŠ¥å‘Š
    [
        daily_kpi_analysis_task,
        customer_segments_analysis_task,
        monthly_trends_analysis_task
    ] >> generate_report_task
    
    # å‘é€é‚®ä»¶å¹¶ç»“æŸ
    generate_report_task >> send_email_task >> end_task