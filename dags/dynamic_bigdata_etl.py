from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.sensors.filesystem import FileSensor
from airflow.operators.dummy import DummyOperator

# å¯¼å…¥å¿…éœ€çš„ providers
try:
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
    SPARK_AVAILABLE = True
except ImportError:
    SPARK_AVAILABLE = False
    print("Warning: SparkSubmitOperator not available")

try:
    from airflow.providers.mysql.operators.mysql import MySqlOperator
    from airflow.providers.mysql.hooks.mysql import MySqlHook
    MYSQL_AVAILABLE = True
except ImportError:
    MYSQL_AVAILABLE = False
    print("Warning: MySQL providers not available")

import os
import logging
import yaml

# å°è¯•åŠ è½½.envæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
def load_env_file():
    """åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡"""
    env_file = '/opt/airflow/.env'
    if os.path.exists(env_file):
        logging.info(f"æ‰¾åˆ°.envæ–‡ä»¶: {env_file}")
        try:
            with open(env_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        # åªè®¾ç½®å°šæœªè®¾ç½®çš„ç¯å¢ƒå˜é‡
                        if key not in os.environ:
                            os.environ[key] = value
                            logging.info(f"ä».envæ–‡ä»¶åŠ è½½: {key}={value}")
            logging.info("æˆåŠŸåŠ è½½.envæ–‡ä»¶")
        except Exception as e:
            logging.warning(f"åŠ è½½.envæ–‡ä»¶å¤±è´¥: {e}")
    else:
        logging.info(f".envæ–‡ä»¶ä¸å­˜åœ¨: {env_file}")

# åŠ è½½ç¯å¢ƒå˜é‡
load_env_file()

def debug_env_vars():
    """è°ƒè¯•ç¯å¢ƒå˜é‡"""
    env_vars = ['EXTERNAL_MYSQL_HOST', 'EXTERNAL_MYSQL_PORT', 'EXTERNAL_MYSQL_USER', 
                'EXTERNAL_MYSQL_PASSWORD', 'EXTERNAL_MYSQL_DATABASE']
    
    logging.info("=== ç¯å¢ƒå˜é‡è°ƒè¯•ä¿¡æ¯ ===")
    for var in env_vars:
        value = os.getenv(var)
        logging.info(f"{var} = {value} (ç±»å‹: {type(value)})")
    logging.info("========================")

def load_dynamic_config():
    """ä»åŠ¨æ€é…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
    config_file = '/opt/airflow/config/production_etl_config_dynamic.yaml'
    
    # é»˜è®¤é…ç½®
    default_config = {
        'mysql_conn': {
            'host': os.getenv('EXTERNAL_MYSQL_HOST', '10.0.19.6'),
            'port': os.getenv('EXTERNAL_MYSQL_PORT', '3306'),  # ä¿æŒä¸ºå­—ç¬¦ä¸²ï¼Œç¨åè½¬æ¢
            'user': os.getenv('EXTERNAL_MYSQL_USER', 'root'),
            'password': os.getenv('EXTERNAL_MYSQL_PASSWORD', 'Sp1derman123@'),
            'database': os.getenv('EXTERNAL_MYSQL_DATABASE', 'wudeli'),
            'connect_timeout': 30,
            'socket_timeout': 60,
            'charset': 'utf8mb4'
        },
        'spark_config': {
            'master': f"spark://{os.getenv('SPARK_MASTER_HOST', 'spark-master')}:{os.getenv('SPARK_MASTER_PORT', '7077')}",
            'deploy_mode': 'client',
            'driver_memory': '2g',
            'executor_memory': os.getenv('SPARK_WORKER_MEMORY', '2g'),
            'executor_cores': os.getenv('SPARK_WORKER_CORES', '2'),  # ä¿æŒä¸ºå­—ç¬¦ä¸²ï¼Œç¨åè½¬æ¢
            'num_executors': 2
        },
        'hdfs_paths': {
            'base': '/user/bigdata/wudeli',
            'raw': '/user/bigdata/wudeli/raw',
            'processed': '/user/bigdata/wudeli/processed',
            'reports': '/user/bigdata/wudeli/reports',
            'temp': '/user/bigdata/wudeli/temp'
        },
        'hive_db': 'wudeli_analytics',
        'services': {
            'hdfs_namenode': f"hdfs://{os.getenv('HDFS_NAMENODE_HOST', 'namenode')}:{os.getenv('HDFS_NAMENODE_PORT', '9000')}",
            'hive_metastore': f"thrift://hive-metastore:{os.getenv('HIVE_METASTORE_PORT', '9083')}"
        }
    }
    
    try:
        # å°è¯•åŠ è½½YAMLé…ç½®æ–‡ä»¶
        if os.path.exists(config_file):
            logging.info(f"æ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_file}")
            with open(config_file, 'r', encoding='utf-8') as f:
                # æ›¿æ¢ç¯å¢ƒå˜é‡
                original_content = f.read()
                logging.info(f"åŸå§‹é…ç½®å†…å®¹å‰100å­—ç¬¦: {original_content[:100]}...")
                
                content = original_content
                # ç¯å¢ƒå˜é‡æ›¿æ¢
                import re
                
                # æ›¿æ¢ ${VAR:-default} æ ¼å¼
                def replace_env_var(match):
                    var_name = match.group(1)
                    default_value = match.group(2)
                    env_value = os.getenv(var_name, default_value)
                    logging.info(f"ç¯å¢ƒå˜é‡æ›¿æ¢: ${{{var_name}:-{default_value}}} -> {env_value}")
                    return env_value
                
                # åŒ¹é… ${VAR:-default} æ ¼å¼
                content = re.sub(r'\$\{([^}:]+):-([^}]+)\}', replace_env_var, content)
                
                # æ›¿æ¢ ${VAR} æ ¼å¼
                def replace_simple_env_var(match):
                    var_name = match.group(1)
                    env_value = os.getenv(var_name, match.group(0))  # å¦‚æœç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œä¿æŒåŸæ ·
                    logging.info(f"ç¯å¢ƒå˜é‡æ›¿æ¢: ${{{var_name}}} -> {env_value}")
                    return env_value
                
                # åŒ¹é… ${VAR} æ ¼å¼
                content = re.sub(r'\$\{([^}:]+)\}', replace_simple_env_var, content)
                
                logging.info(f"æ›¿æ¢åé…ç½®å†…å®¹å‰100å­—ç¬¦: {content[:100]}...")
                
                yaml_config = yaml.safe_load(content)
                logging.info(f"YAMLè§£æç»“æœ: {yaml_config}")
                
                # åˆå¹¶é…ç½®
                if yaml_config:
                    # æ·±åº¦åˆå¹¶é…ç½®
                    def deep_merge(base, update):
                        for key, value in update.items():
                            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                                deep_merge(base[key], value)
                            else:
                                base[key] = value
                    
                    deep_merge(default_config, yaml_config)
                    logging.info("æˆåŠŸåŠ è½½åŠ¨æ€é…ç½®æ–‡ä»¶")
        else:
            logging.info(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    except Exception as e:
        logging.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {str(e)}")
        import traceback
        logging.warning(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
    
    # ç±»å‹è½¬æ¢ - ç¡®ä¿æ•°å€¼ç±»å‹æ­£ç¡®
    try:
        # è½¬æ¢MySQLç«¯å£
        if 'mysql_conn' in default_config and 'port' in default_config['mysql_conn']:
            port_value = default_config['mysql_conn']['port']
            if isinstance(port_value, str):
                default_config['mysql_conn']['port'] = int(port_value)
            elif not isinstance(port_value, int):
                default_config['mysql_conn']['port'] = 3306
                logging.warning(f"MySQLç«¯å£å€¼æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼3306: {port_value}")
        
        # è½¬æ¢Sparké…ç½®
        if 'spark_config' in default_config:
            spark_config = default_config['spark_config']
            if 'executor_cores' in spark_config:
                cores_value = spark_config['executor_cores']
                if isinstance(cores_value, str):
                    spark_config['executor_cores'] = int(cores_value)
                elif not isinstance(cores_value, int):
                    spark_config['executor_cores'] = 2
                    logging.warning(f"Spark executor_coreså€¼æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼2: {cores_value}")
            
            if 'num_executors' in spark_config:
                executors_value = spark_config['num_executors']
                if isinstance(executors_value, str):
                    spark_config['num_executors'] = int(executors_value)
                elif not isinstance(executors_value, int):
                    spark_config['num_executors'] = 2
                    logging.warning(f"Spark num_executorså€¼æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼2: {executors_value}")
                    
    except (ValueError, TypeError) as e:
        logging.error(f"é…ç½®ç±»å‹è½¬æ¢å¤±è´¥: {str(e)}")
        # è®¾ç½®å®‰å…¨çš„é»˜è®¤å€¼
        default_config['mysql_conn']['port'] = 3306
        default_config['spark_config']['executor_cores'] = 2
        default_config['spark_config']['num_executors'] = 2
    
    # æœ€ç»ˆé…ç½®è°ƒè¯•ä¿¡æ¯
    logging.info("=== æœ€ç»ˆé…ç½®ä¿¡æ¯ ===")
    mysql_config = default_config.get('mysql_conn', {})
    logging.info(f"MySQL: host={mysql_config.get('host')}, port={mysql_config.get('port')} (ç±»å‹:{type(mysql_config.get('port'))})")
    logging.info("==================")
    
    return default_config

# è°ƒè¯•ç¯å¢ƒå˜é‡
debug_env_vars()

# åŠ è½½åŠ¨æ€é…ç½®
PRODUCTION_CONFIG = load_dynamic_config()

# éªŒè¯é…ç½®
def validate_config():
    """éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½"""
    mysql_config = PRODUCTION_CONFIG.get('mysql_conn', {})
    logging.info(f"MySQLé…ç½®: Host={mysql_config.get('host')}, Port={mysql_config.get('port')} (ç±»å‹: {type(mysql_config.get('port'))}), Database={mysql_config.get('database')}")
    
    spark_config = PRODUCTION_CONFIG.get('spark_config', {})
    logging.info(f"Sparké…ç½®: Master={spark_config.get('master')}, Executor Cores={spark_config.get('executor_cores')} (ç±»å‹: {type(spark_config.get('executor_cores'))})")
    
    # éªŒè¯å…³é”®é…ç½®é¡¹çš„ç±»å‹
    try:
        port = mysql_config.get('port')
        if port is not None and not isinstance(port, int):
            logging.warning(f"MySQLç«¯å£ä¸æ˜¯æ•´æ•°ç±»å‹: {port} (ç±»å‹: {type(port)})")
        
        cores = spark_config.get('executor_cores')
        if cores is not None and not isinstance(cores, int):
            logging.warning(f"Spark executor_coresä¸æ˜¯æ•´æ•°ç±»å‹: {cores} (ç±»å‹: {type(cores)})")
            
    except Exception as e:
        logging.error(f"é…ç½®éªŒè¯æ—¶å‡ºé”™: {e}")
    
    return True

# éªŒè¯é…ç½®
validate_config()

default_args = {
    'owner': 'bigdata_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': int(os.getenv('ETL_RETRY_ATTEMPTS', 2)),
    'retry_delay': timedelta(minutes=int(os.getenv('ETL_RETRY_DELAY', 10))),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(hours=1),
    'email_on_failure': os.getenv('ENABLE_ALERTS', 'true').lower() == 'true',
    'email_on_retry': False,
    'execution_timeout': timedelta(hours=2),  # å¢åŠ ä»»åŠ¡è¶…æ—¶æ—¶é—´åˆ°2å°æ—¶
}

def check_mysql_connection(**context):
    """æ£€æŸ¥MySQLè¿æ¥å’Œæ•°æ®"""
    import mysql.connector
    import logging
    
    config = PRODUCTION_CONFIG['mysql_conn']
    
    # éªŒè¯é…ç½®
    logging.info(f"MySQLè¿æ¥é…ç½®: Host={config['host']}, Port={config['port']} (ç±»å‹: {type(config['port'])}), Database={config['database']}")
    
    # ç¡®ä¿ç«¯å£æ˜¯æ•´æ•°
    try:
        port = int(config['port']) if not isinstance(config['port'], int) else config['port']
    except (ValueError, TypeError):
        logging.error(f"æ— æ•ˆçš„ç«¯å£å€¼: {config['port']}")
        raise ValueError(f"MySQLç«¯å£å¿…é¡»æ˜¯æ•´æ•°ï¼Œå½“å‰å€¼: {config['port']}")
    
    try:
        connection = mysql.connector.connect(
            host=config['host'],
            port=port,
            user=config['user'],
            password=config['password'],
            database=config['database'],
            connection_timeout=config.get('connect_timeout', 30),
            charset=config.get('charset', 'utf8mb4')
        )
        
        cursor = connection.cursor()
        
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        cursor.execute("SELECT 1")
        cursor.fetchone()
        logging.info("MySQLæ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„è¡¨
        cursor.execute("SHOW TABLES")
        tables = cursor.fetchall()
        table_names = [table[0] for table in tables] if tables else []
        logging.info(f"æ•°æ®åº“ {config['database']} ä¸­çš„è¡¨ ({len(table_names)}): {table_names}")
        
        if tables:
            # æ£€æŸ¥å¸¸è§è¡¨çš„æ•°æ®
            common_tables = ['orders', 'customers', 'products']
            for table_name in common_tables:
                if table_name in table_names:
                    try:
                        cursor.execute(f"SELECT COUNT(*) as count FROM {table_name}")
                        result = cursor.fetchone()
                        logging.info(f"{table_name}è¡¨è®°å½•æ•°: {result[0]}")
                        
                        # å°è¯•æ£€æŸ¥æœ€æ–°æ•°æ®ï¼ˆå‡è®¾æœ‰created_atæˆ–updated_atå­—æ®µï¼‰
                        for date_col in ['created_at', 'updated_at', 'UpdatedDate']:
                            try:
                                cursor.execute(f"SELECT MAX({date_col}) FROM {table_name}")
                                latest = cursor.fetchone()
                                if latest[0]:
                                    logging.info(f"{table_name}è¡¨æœ€æ–°{date_col}: {latest[0]}")
                                    break
                            except mysql.connector.Error:
                                continue  # å­—æ®µä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ª
                                
                    except mysql.connector.Error as table_error:
                        logging.warning(f"æ£€æŸ¥è¡¨ {table_name} æ—¶å‡ºé”™: {table_error}")
        else:
            logging.warning("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨")
        
        cursor.close()
        connection.close()
        
        logging.info("MySQLè¿æ¥æ£€æŸ¥å®Œæˆ")
        return True
        
    except mysql.connector.Error as mysql_error:
        logging.error(f"MySQLè¿æ¥é”™è¯¯: {mysql_error}")
        raise
    except Exception as e:
        logging.error(f"MySQLè¿æ¥æ£€æŸ¥å¤±è´¥: {str(e)}")
        raise

def check_hdfs_connection(**context):
    """æ£€æŸ¥HDFSè¿æ¥ - åŸºäºWeb APIçš„Dockerç¯å¢ƒç‰ˆæœ¬"""
    import logging
    import requests
    import json
    import time
    from datetime import datetime
    
    try:
        # åœ¨Dockerç½‘ç»œä¸­ï¼Œä½¿ç”¨æœåŠ¡åè®¿é—®
        namenode_web_url = "http://namenode:9870"
        webhdfs_base_url = f"{namenode_web_url}/webhdfs/v1"
        
        checks_passed = 0
        total_checks = 0
        
        # 1. æ£€æŸ¥NameNode Web UIæ˜¯å¦å¯è®¿é—®
        total_checks += 1
        try:
            response = requests.get(f"{namenode_web_url}/", timeout=10)
            if response.status_code == 200:
                logging.info("âœ“ NameNode Web UIå¯è®¿é—®")
                checks_passed += 1
            else:
                logging.error(f"âœ— NameNode Web UIè¿”å›é”™è¯¯çŠ¶æ€: {response.status_code}")
        except requests.exceptions.RequestException as e:
            logging.error(f"âœ— NameNode Web UIè¿æ¥å¤±è´¥: {e}")
        
        # 2. æ£€æŸ¥WebHDFS API - åˆ—å‡ºæ ¹ç›®å½•
        total_checks += 1
        try:
            response = requests.get(f"{webhdfs_base_url}/?op=LISTSTATUS", timeout=10)
            if response.status_code == 200:
                data = response.json()
                file_count = len(data.get('FileStatuses', {}).get('FileStatus', []))
                logging.info(f"âœ“ WebHDFS APIå¯è®¿é—®ï¼Œæ ¹ç›®å½•åŒ…å« {file_count} ä¸ªé¡¹ç›®")
                checks_passed += 1
            else:
                logging.error(f"âœ— WebHDFS APIè¿”å›é”™è¯¯çŠ¶æ€: {response.status_code}")
        except requests.exceptions.RequestException as e:
            logging.error(f"âœ— WebHDFS APIè¿æ¥å¤±è´¥: {e}")
        except json.JSONDecodeError as e:
            logging.error(f"âœ— WebHDFS APIå“åº”è§£æå¤±è´¥: {e}")
        
        # 3. æ£€æŸ¥HDFSé›†ç¾¤å¥åº·çŠ¶æ€
        total_checks += 1
        try:
            response = requests.get(f"{namenode_web_url}/jmx?qry=Hadoop:service=NameNode,name=FSNamesystemState", timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data.get('beans'):
                    bean = data['beans'][0]
                    total_blocks = bean.get('TotalBlocks', 0)
                    missing_blocks = bean.get('MissingBlocks', 0)
                    corrupt_blocks = bean.get('CorruptBlocks', 0)
                    live_nodes = bean.get('NumLiveDataNodes', 0)
                    
                    logging.info(f"âœ“ HDFSé›†ç¾¤çŠ¶æ€: æ€»å—æ•°={total_blocks}, ä¸¢å¤±å—={missing_blocks}, æŸåå—={corrupt_blocks}, æ´»è·ƒèŠ‚ç‚¹={live_nodes}")
                    
                    if missing_blocks == 0 and corrupt_blocks == 0 and live_nodes > 0:
                        logging.info("âœ“ HDFSé›†ç¾¤å¥åº·çŠ¶æ€è‰¯å¥½")
                        checks_passed += 1
                    else:
                        logging.warning("âš  HDFSé›†ç¾¤å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œä½†å¯ä»¥ç»§ç»­")
                        checks_passed += 0.5  # éƒ¨åˆ†é€šè¿‡
                else:
                    logging.error("âœ— æ— æ³•è·å–HDFSé›†ç¾¤çŠ¶æ€ä¿¡æ¯")
            else:
                logging.error(f"âœ— HDFSé›†ç¾¤çŠ¶æ€æ£€æŸ¥å¤±è´¥: {response.status_code}")
        except Exception as e:
            logging.error(f"âœ— HDFSé›†ç¾¤çŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
        
        # 4. æ£€æŸ¥å’Œåˆ›å»ºå¿…è¦ç›®å½•
        total_checks += 1
        hdfs_paths = PRODUCTION_CONFIG['hdfs_paths']
        dirs_available = 0
        
        for path_name, path_value in hdfs_paths.items():
            try:
                # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
                response = requests.get(f"{webhdfs_base_url}{path_value}?op=LISTSTATUS", timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    file_count = len(data.get('FileStatuses', {}).get('FileStatus', []))
                    logging.info(f"âœ“ HDFSç›®å½• {path_name} ({path_value}) å­˜åœ¨ï¼ŒåŒ…å« {file_count} ä¸ªé¡¹ç›®")
                    dirs_available += 1
                elif response.status_code == 404:
                    # ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º
                    logging.warning(f"âš  HDFSç›®å½• {path_name} ({path_value}) ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º")
                    create_response = requests.put(f"{webhdfs_base_url}{path_value}?op=MKDIRS", timeout=10)
                    if create_response.status_code == 200:
                        create_data = create_response.json()
                        if create_data.get('boolean', False):
                            logging.info(f"âœ“ æˆåŠŸåˆ›å»ºHDFSç›®å½•: {path_value}")
                            dirs_available += 1
                        else:
                            logging.warning(f"âœ— åˆ›å»ºHDFSç›®å½•å¤±è´¥: {path_value}")
                    else:
                        logging.warning(f"âœ— åˆ›å»ºHDFSç›®å½•è¯·æ±‚å¤±è´¥: {path_value}, çŠ¶æ€ç : {create_response.status_code}")
                else:
                    logging.warning(f"âœ— æ£€æŸ¥HDFSç›®å½•å¤±è´¥: {path_value}, çŠ¶æ€ç : {response.status_code}")
            except Exception as e:
                logging.warning(f"âœ— æ£€æŸ¥HDFSç›®å½•å¼‚å¸¸: {path_value}, é”™è¯¯: {e}")
        
        if dirs_available >= len(hdfs_paths) * 0.7:  # 70%çš„ç›®å½•å¯ç”¨
            logging.info("âœ“ HDFSç›®å½•æ£€æŸ¥é€šè¿‡")
            checks_passed += 1
        else:
            logging.error("âœ— HDFSç›®å½•æ£€æŸ¥å¤±è´¥")
        
        # 5. ç®€å•çš„è¯»å†™æµ‹è¯•
        total_checks += 1
        test_dir = "/tmp/hdfs_connection_test"
        test_file = f"{test_dir}/test_{int(time.time())}.txt"
        test_content = f"HDFS connection test at {datetime.now()}"
        
        try:
            # åˆ›å»ºæµ‹è¯•ç›®å½•
            create_response = requests.put(f"{webhdfs_base_url}{test_dir}?op=MKDIRS", timeout=10)
            if create_response.status_code == 200:
                # å†™å…¥æµ‹è¯•æ–‡ä»¶
                write_response = requests.put(
                    f"{webhdfs_base_url}{test_file}?op=CREATE&overwrite=true",
                    data=test_content,
                    timeout=10,
                    allow_redirects=True
                )
                
                if write_response.status_code in [200, 201]:
                    # è¯»å–æµ‹è¯•æ–‡ä»¶
                    read_response = requests.get(f"{webhdfs_base_url}{test_file}?op=OPEN", timeout=10)
                    if read_response.status_code == 200 and test_content in read_response.text:
                        logging.info("âœ“ HDFSè¯»å†™æµ‹è¯•é€šè¿‡")
                        checks_passed += 1
                        
                        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
                        delete_response = requests.delete(f"{webhdfs_base_url}{test_dir}?op=DELETE&recursive=true", timeout=10)
                        if delete_response.status_code == 200:
                            logging.info("âœ“ æ¸…ç†æµ‹è¯•æ–‡ä»¶å®Œæˆ")
                    else:
                        logging.error("âœ— HDFSè¯»å–æµ‹è¯•å¤±è´¥")
                else:
                    logging.error(f"âœ— HDFSå†™å…¥æµ‹è¯•å¤±è´¥ï¼ŒçŠ¶æ€ç : {write_response.status_code}")
            else:
                logging.error(f"âœ— HDFSåˆ›å»ºæµ‹è¯•ç›®å½•å¤±è´¥ï¼ŒçŠ¶æ€ç : {create_response.status_code}")
        except Exception as e:
            logging.error(f"âœ— HDFSè¯»å†™æµ‹è¯•å¼‚å¸¸: {e}")
        
        # è®¡ç®—é€šè¿‡ç‡
        pass_rate = (checks_passed / total_checks) * 100 if total_checks > 0 else 0
        
        logging.info(f"HDFSè¿æ¥æ£€æŸ¥å®Œæˆ: {checks_passed}/{total_checks} é¡¹é€šè¿‡ ({pass_rate:.1f}%)")
        
        # 60%é€šè¿‡ç‡è®¤ä¸ºè¿æ¥åŸºæœ¬æ­£å¸¸ï¼ˆé™ä½è¦æ±‚ï¼Œå› ä¸ºWeb APIå¯èƒ½æœ‰é™åˆ¶ï¼‰
        if pass_rate >= 60:
            logging.info("ğŸ‰ HDFSè¿æ¥çŠ¶æ€: è‰¯å¥½")
            return True
        else:
            logging.error("âŒ HDFSè¿æ¥çŠ¶æ€: å­˜åœ¨é—®é¢˜")
            # åœ¨Dockerç¯å¢ƒä¸­ï¼Œå³ä½¿éƒ¨åˆ†æ£€æŸ¥å¤±è´¥ä¹Ÿç»§ç»­æ‰§è¡Œï¼Œé¿å…é˜»å¡ETLæµç¨‹
            logging.warning("ç»§ç»­æ‰§è¡ŒETLæµç¨‹ï¼Œä½†å»ºè®®æ£€æŸ¥HDFSé…ç½®")
            return True
            
    except Exception as e:
        logging.error(f"HDFSè¿æ¥æ£€æŸ¥å¤±è´¥: {str(e)}")
        # éè‡´å‘½é”™è¯¯ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­æ‰§è¡Œ
        logging.warning("HDFSè¿æ¥æ£€æŸ¥å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡ŒETLæµç¨‹")
        return True

def check_spark_connection(**context):
    """æ£€æŸ¥Sparkè¿æ¥ - åŸºäºWeb APIçš„Dockerç¯å¢ƒç‰ˆæœ¬"""
    import logging
    import requests
    import json
    
    try:
        # åœ¨Dockerç½‘ç»œä¸­ï¼Œä½¿ç”¨æœåŠ¡åè®¿é—®
        spark_master_url = "http://spark-master:8080"
        
        checks_passed = 0
        total_checks = 0
        
        # 1. æ£€æŸ¥Spark Master Web UIæ˜¯å¦å¯è®¿é—®
        total_checks += 1
        try:
            response = requests.get(f"{spark_master_url}/", timeout=10)
            if response.status_code == 200:
                logging.info("âœ“ Spark Master Web UIå¯è®¿é—®")
                checks_passed += 1
            else:
                logging.error(f"âœ— Spark Master Web UIè¿”å›é”™è¯¯çŠ¶æ€: {response.status_code}")
        except requests.exceptions.RequestException as e:
            logging.error(f"âœ— Spark Master Web UIè¿æ¥å¤±è´¥: {e}")
        
        # 2. æ£€æŸ¥Spark Master API - è·å–é›†ç¾¤çŠ¶æ€
        total_checks += 1
        try:
            response = requests.get(f"{spark_master_url}/json/", timeout=10)
            if response.status_code == 200:
                data = response.json()
                status = data.get('status', 'UNKNOWN')
                workers = data.get('workers', [])
                alive_workers = data.get('aliveworkers', 0)
                
                logging.info(f"âœ“ Spark Master APIå¯è®¿é—®ï¼ŒçŠ¶æ€: {status}")
                logging.info(f"âœ“ æ´»è·ƒWorkeræ•°é‡: {alive_workers}")
                
                if status == 'ALIVE' and alive_workers > 0:
                    logging.info("âœ“ Sparké›†ç¾¤çŠ¶æ€è‰¯å¥½")
                    checks_passed += 1
                elif status == 'ALIVE':
                    logging.warning("âš  Spark Masteræ­£å¸¸ä½†æ²¡æœ‰WorkerèŠ‚ç‚¹")
                    checks_passed += 0.5  # éƒ¨åˆ†é€šè¿‡
                else:
                    logging.error(f"âœ— Spark MasterçŠ¶æ€å¼‚å¸¸: {status}")
            else:
                logging.error(f"âœ— Spark Master APIè¿”å›é”™è¯¯çŠ¶æ€: {response.status_code}")
        except requests.exceptions.RequestException as e:
            logging.error(f"âœ— Spark Master APIè¿æ¥å¤±è´¥: {e}")
        except json.JSONDecodeError as e:
            logging.error(f"âœ— Spark Master APIå“åº”è§£æå¤±è´¥: {e}")
        
        # 3. æ£€æŸ¥Sparkåº”ç”¨ç¨‹åºçŠ¶æ€
        total_checks += 1
        try:
            response = requests.get(f"{spark_master_url}/json/", timeout=10)
            if response.status_code == 200:
                data = response.json()
                completed_apps = data.get('completedapps', [])
                active_apps = data.get('activeapps', [])
                
                logging.info(f"âœ“ æ´»è·ƒåº”ç”¨ç¨‹åº: {len(active_apps)} ä¸ª")
                logging.info(f"âœ“ å·²å®Œæˆåº”ç”¨ç¨‹åº: {len(completed_apps)} ä¸ª")
                
                # å¦‚æœèƒ½è·å–åˆ°åº”ç”¨ç¨‹åºä¿¡æ¯ï¼Œè¯´æ˜APIæ­£å¸¸å·¥ä½œ
                logging.info("âœ“ Sparkåº”ç”¨ç¨‹åºçŠ¶æ€æ£€æŸ¥é€šè¿‡")
                checks_passed += 1
            else:
                logging.error(f"âœ— æ— æ³•è·å–Sparkåº”ç”¨ç¨‹åºçŠ¶æ€: {response.status_code}")
        except Exception as e:
            logging.error(f"âœ— Sparkåº”ç”¨ç¨‹åºçŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
        
        # 4. æ£€æŸ¥WorkerèŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
        total_checks += 1
        try:
            response = requests.get(f"{spark_master_url}/json/", timeout=10)
            if response.status_code == 200:
                data = response.json()
                workers = data.get('workers', [])
                
                if workers:
                    for i, worker in enumerate(workers):
                        worker_id = worker.get('id', f'worker-{i}')
                        worker_state = worker.get('state', 'UNKNOWN')
                        worker_cores = worker.get('cores', 0)
                        worker_memory = worker.get('memory', 0)
                        
                        logging.info(f"âœ“ Worker {worker_id}: çŠ¶æ€={worker_state}, æ ¸å¿ƒæ•°={worker_cores}, å†…å­˜={worker_memory}MB")
                    
                    alive_workers = len([w for w in workers if w.get('state') == 'ALIVE'])
                    if alive_workers > 0:
                        logging.info(f"âœ“ WorkerèŠ‚ç‚¹æ£€æŸ¥é€šè¿‡ï¼Œ{alive_workers} ä¸ªèŠ‚ç‚¹æ­£å¸¸")
                        checks_passed += 1
                    else:
                        logging.warning("âš  æ²¡æœ‰æ´»è·ƒçš„WorkerèŠ‚ç‚¹")
                        checks_passed += 0.5
                else:
                    logging.warning("âš  æ²¡æœ‰å‘ç°WorkerèŠ‚ç‚¹")
                    checks_passed += 0.5
            else:
                logging.error(f"âœ— æ— æ³•è·å–WorkerèŠ‚ç‚¹ä¿¡æ¯: {response.status_code}")
        except Exception as e:
            logging.error(f"âœ— WorkerèŠ‚ç‚¹æ£€æŸ¥å¼‚å¸¸: {e}")
        
        # 5. æ£€æŸ¥Spark History Serverï¼ˆå¦‚æœå¯ç”¨ï¼‰
        total_checks += 1
        try:
            # å°è¯•è®¿é—®History Server
            history_response = requests.get("http://spark-master:18080/", timeout=5)
            if history_response.status_code == 200:
                logging.info("âœ“ Spark History Serverå¯è®¿é—®")
                checks_passed += 1
            else:
                logging.info("â„¹ Spark History Serverä¸å¯ç”¨ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰")
                checks_passed += 0.5  # ä¸æ˜¯å¿…éœ€çš„ï¼Œç»™éƒ¨åˆ†åˆ†æ•°
        except requests.exceptions.RequestException:
            logging.info("â„¹ Spark History Serverä¸å¯ç”¨ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰")
            checks_passed += 0.5  # ä¸æ˜¯å¿…éœ€çš„ï¼Œç»™éƒ¨åˆ†åˆ†æ•°
        
        # è®¡ç®—é€šè¿‡ç‡
        pass_rate = (checks_passed / total_checks) * 100 if total_checks > 0 else 0
        
        logging.info(f"Sparkè¿æ¥æ£€æŸ¥å®Œæˆ: {checks_passed}/{total_checks} é¡¹é€šè¿‡ ({pass_rate:.1f}%)")
        
        # 60%é€šè¿‡ç‡è®¤ä¸ºè¿æ¥åŸºæœ¬æ­£å¸¸
        if pass_rate >= 60:
            logging.info("ğŸ‰ Sparkè¿æ¥çŠ¶æ€: è‰¯å¥½")
            return True
        else:
            logging.error("âŒ Sparkè¿æ¥çŠ¶æ€: å­˜åœ¨é—®é¢˜")
            # åœ¨Dockerç¯å¢ƒä¸­ï¼Œå³ä½¿éƒ¨åˆ†æ£€æŸ¥å¤±è´¥ä¹Ÿç»§ç»­æ‰§è¡Œï¼Œé¿å…é˜»å¡ETLæµç¨‹
            logging.warning("ç»§ç»­æ‰§è¡ŒETLæµç¨‹ï¼Œä½†å»ºè®®æ£€æŸ¥Sparké…ç½®")
            return True
            
    except Exception as e:
        logging.error(f"Sparkè¿æ¥æ£€æŸ¥å¤±è´¥: {str(e)}")
        # éè‡´å‘½é”™è¯¯ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­æ‰§è¡Œ
        logging.warning("Sparkè¿æ¥æ£€æŸ¥å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡ŒETLæµç¨‹")
        return True

def run_mysql_to_hive_etl(**context):
    """æ‰§è¡ŒMySQLåˆ°Hiveçš„ETLä½œä¸š - ç›´æ¥è°ƒç”¨åŠ¨æ€è„šæœ¬"""
    import logging
    import sys
    import os
    
    try:
        logging.info("å¼€å§‹æ‰§è¡ŒMySQLåˆ°Hive ETLä½œä¸š")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿åŠ¨æ€è„šæœ¬èƒ½è·å–åˆ°é…ç½®
        config = PRODUCTION_CONFIG
        mysql_config = config['mysql_conn']
        
        # å¯¼å‡ºç¯å¢ƒå˜é‡ä¾›åŠ¨æ€è„šæœ¬ä½¿ç”¨
        os.environ['EXTERNAL_MYSQL_HOST'] = mysql_config['host']
        os.environ['EXTERNAL_MYSQL_PORT'] = str(mysql_config['port'])
        os.environ['EXTERNAL_MYSQL_USER'] = mysql_config['user']
        os.environ['EXTERNAL_MYSQL_PASSWORD'] = mysql_config['password']
        os.environ['EXTERNAL_MYSQL_DATABASE'] = mysql_config['database']
        
        logging.info(f"é…ç½®MySQLè¿æ¥: {mysql_config['host']}:{mysql_config['port']}/{mysql_config['database']}")
        
        # ç›´æ¥åœ¨å½“å‰ç¯å¢ƒæ‰§è¡ŒåŠ¨æ€è„šæœ¬
        try:
            # æ·»åŠ è„šæœ¬è·¯å¾„åˆ°Pythonè·¯å¾„
            script_path = '/opt/airflow/spark_jobs'
            if script_path not in sys.path:
                sys.path.insert(0, script_path)
            
            # å¯¼å…¥å¹¶æ‰§è¡ŒåŠ¨æ€ETLè„šæœ¬
            logging.info("å¯¼å…¥å¹¶æ‰§è¡ŒåŠ¨æ€MySQLåˆ°Hive ETLè„šæœ¬")
            
            # åŠ¨æ€å¯¼å…¥æ¨¡å—
            import importlib.util
            spec = importlib.util.spec_from_file_location(
                "dynamic_mysql_to_hive", 
                "/opt/airflow/spark_jobs/dynamic_mysql_to_hive.py"
            )
            etl_module = importlib.util.module_from_spec(spec)
            
            # æ‰§è¡Œæ¨¡å—
            spec.loader.exec_module(etl_module)
            
            # è°ƒç”¨ä¸»å‡½æ•°
            etl_module.main()
            
            logging.info("MySQLåˆ°Hive ETLä½œä¸šæ‰§è¡ŒæˆåŠŸ")
            return True
            
        except Exception as direct_error:
            logging.error(f"ç›´æ¥æ‰§è¡Œå¤±è´¥: {direct_error}")
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç®€åŒ–çš„ETLé€»è¾‘
            logging.info("ä½¿ç”¨å¤‡ç”¨ETLé€»è¾‘")
            return run_simplified_etl(config)
            
    except Exception as e:
        logging.error(f"MySQLåˆ°Hive ETLä½œä¸šå¤±è´¥: {str(e)}")
        raise

def run_simplified_etl(config):
    """ç®€åŒ–çš„ETLé€»è¾‘ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ"""
    import logging
    
    try:
        from pyspark.sql import SparkSession
        
        logging.info("å¯åŠ¨ç®€åŒ–ETLæµç¨‹")
        
        # åˆ›å»ºSparkä¼šè¯
        spark = SparkSession.builder \
            .appName("Simplified_MySQL_to_Hive_ETL") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
            .enableHiveSupport() \
            .getOrCreate()
        
        mysql_config = config['mysql_conn']
        jdbc_url = f"jdbc:mysql://{mysql_config['host']}:{mysql_config['port']}/{mysql_config['database']}"
        
        # ç®€åŒ–çš„è¡¨åˆ—è¡¨
        tables_to_process = ['customers', 'orders', 'products']
        hive_db = config.get('hive_db', 'wudeli_analytics')
        
        # åˆ›å»ºHiveæ•°æ®åº“
        spark.sql(f"CREATE DATABASE IF NOT EXISTS {hive_db}")
        spark.sql(f"USE {hive_db}")
        
        processed_count = 0
        
        for table_name in tables_to_process:
            try:
                logging.info(f"å¤„ç†è¡¨: {table_name}")
                
                # ä»MySQLè¯»å–æ•°æ®
                df = spark.read \
                    .format("jdbc") \
                    .option("url", jdbc_url) \
                    .option("dbtable", table_name) \
                    .option("user", mysql_config['user']) \
                    .option("password", mysql_config['password']) \
                    .option("driver", "com.mysql.cj.jdbc.Driver") \
                    .load()
                
                record_count = df.count()
                if record_count > 0:
                    # å†™å…¥Hiveè¡¨
                    hive_table = f"{hive_db}.{table_name}"
                    df.write \
                        .mode("overwrite") \
                        .saveAsTable(hive_table)
                    
                    logging.info(f"è¡¨ {table_name} å¤„ç†å®Œæˆï¼Œè®°å½•æ•°: {record_count}")
                    processed_count += 1
                else:
                    logging.warning(f"è¡¨ {table_name} æ²¡æœ‰æ•°æ®")
                    
            except Exception as table_error:
                logging.error(f"å¤„ç†è¡¨ {table_name} å¤±è´¥: {table_error}")
                continue
        
        spark.stop()
        
        if processed_count > 0:
            logging.info(f"ç®€åŒ–ETLå®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count} ä¸ªè¡¨")
            return True
        else:
            raise Exception("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•è¡¨")
            
    except Exception as e:
        logging.error(f"ç®€åŒ–ETLå¤±è´¥: {e}")
        raise

def create_hive_analytics_views(**context):
    """åˆ›å»ºHiveåˆ†æè§†å›¾"""
    import subprocess
    import logging
    
    try:
        config = PRODUCTION_CONFIG
        
        # æ„å»ºspark-submitå‘½ä»¤æ¥åˆ›å»ºåˆ†æè§†å›¾
        spark_cmd = [
            'spark-submit',
            '--master', config['spark_config']['master'],
            '--deploy-mode', config['spark_config']['deploy_mode'],
            '--driver-memory', config['spark_config']['driver_memory'],
            '--executor-memory', config['spark_config']['executor_memory'],
            '/opt/airflow/spark_jobs/hive_analytics_views.py'
        ]
        
        logging.info(f"åˆ›å»ºHiveåˆ†æè§†å›¾: {' '.join(spark_cmd)}")
        
        result = subprocess.run(spark_cmd, capture_output=True, text=True, timeout=900)  # 15åˆ†é’Ÿè¶…æ—¶
        
        if result.returncode == 0:
            logging.info("Hiveåˆ†æè§†å›¾åˆ›å»ºå®Œæˆ")
            logging.info(f"è§†å›¾åˆ›å»ºè¾“å‡º: {result.stdout}")
            return True
        else:
            logging.error(f"Hiveåˆ†æè§†å›¾åˆ›å»ºå¤±è´¥: {result.stderr}")
            raise Exception(f"è§†å›¾åˆ›å»ºå¤±è´¥: {result.stderr}")
            
    except subprocess.TimeoutExpired:
        logging.error("Hiveåˆ†æè§†å›¾åˆ›å»ºè¶…æ—¶")
        raise Exception("è§†å›¾åˆ›å»ºè¶…æ—¶")
    except Exception as e:
        logging.error(f"Hiveåˆ†æè§†å›¾åˆ›å»ºå¤±è´¥: {str(e)}")
        raise

# åˆ›å»ºDAG
dag = DAG(
    'dynamic_bigdata_etl',
    default_args=default_args,
    description='åŠ¨æ€é…ç½®çš„å¤§æ•°æ®ETLæµç¨‹',
    schedule_interval=os.getenv('ETL_SCHEDULE_CRON', '0 2 * * *'),
    max_active_runs=1,
    catchup=False,
    tags=['etl', 'bigdata', 'production', 'dynamic']
)

# ä»»åŠ¡å®šä¹‰
start_task = DummyOperator(
    task_id='start_etl_pipeline',
    dag=dag
)

# è¿æ¥æ£€æŸ¥ä»»åŠ¡
check_mysql = PythonOperator(
    task_id='check_mysql_connection',
    python_callable=check_mysql_connection,
    dag=dag
)

check_hdfs = PythonOperator(
    task_id='check_hdfs_connection',
    python_callable=check_hdfs_connection,
    dag=dag
)

check_spark = PythonOperator(
    task_id='check_spark_connection',
    python_callable=check_spark_connection,
    dag=dag
)

# æ•°æ®åŒæ­¥ä»»åŠ¡
mysql_to_hive = PythonOperator(
    task_id='mysql_to_hive_etl',
    python_callable=run_mysql_to_hive_etl,
    dag=dag
)

# åˆ›å»ºåˆ†æè§†å›¾
create_views = PythonOperator(
    task_id='create_analytics_views',
    python_callable=create_hive_analytics_views,
    dag=dag
)

# æ•°æ®è´¨é‡æ£€æŸ¥
data_quality_check = BashOperator(
    task_id='data_quality_check',
    bash_command=f"""
    echo "æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥..."
    
    # æ£€æŸ¥HDFSä¸­çš„æ•°æ®æ–‡ä»¶
    docker compose exec namenode hdfs dfs -ls {PRODUCTION_CONFIG['hdfs_paths']['processed']} || echo "å¤„ç†åæ•°æ®ç›®å½•ä¸ºç©º"
    
    # æ£€æŸ¥æ•°æ®æ–°é²œåº¦ï¼ˆè¿™é‡Œæ˜¯ç¤ºä¾‹ï¼Œå®é™…åº”è¯¥æ£€æŸ¥å…·ä½“çš„æ•°æ®æ—¶é—´æˆ³ï¼‰
    echo "æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ"
    """,
    dag=dag
)

# ç”ŸæˆæŠ¥å‘Š
generate_report = BashOperator(
    task_id='generate_etl_report',
    bash_command=f"""
    echo "ç”ŸæˆETLæŠ¥å‘Š..."
    
    # åˆ›å»ºæŠ¥å‘Šç›®å½•
    docker compose exec namenode hdfs dfs -mkdir -p {PRODUCTION_CONFIG['hdfs_paths']['reports']}/$(date +%Y%m%d)
    
    # ç”Ÿæˆç®€å•çš„æŠ¥å‘Šï¼ˆå®é™…ä¸­åº”è¯¥æ˜¯æ›´å¤æ‚çš„æŠ¥å‘Šç”Ÿæˆé€»è¾‘ï¼‰
    echo "ETLæ‰§è¡ŒæŠ¥å‘Š - $(date)" > /tmp/etl_report.txt
    echo "å¤„ç†è®°å½•æ•°: å¾…ç»Ÿè®¡" >> /tmp/etl_report.txt
    echo "æ‰§è¡ŒçŠ¶æ€: æˆåŠŸ" >> /tmp/etl_report.txt
    
    # å°†æŠ¥å‘Šå¤åˆ¶åˆ°namenodeå®¹å™¨å¹¶ä¸Šä¼ åˆ°HDFS
    docker cp /tmp/etl_report.txt $(docker compose ps -q namenode):/tmp/etl_report.txt
    docker compose exec namenode hdfs dfs -put /tmp/etl_report.txt {PRODUCTION_CONFIG['hdfs_paths']['reports']}/$(date +%Y%m%d)/etl_report.txt
    
    echo "ETLæŠ¥å‘Šç”Ÿæˆå®Œæˆ"
    """,
    dag=dag
)

end_task = DummyOperator(
    task_id='end_etl_pipeline',
    dag=dag
)

# å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»
start_task >> [check_mysql, check_hdfs, check_spark]
[check_mysql, check_hdfs, check_spark] >> mysql_to_hive
mysql_to_hive >> create_views
create_views >> data_quality_check
data_quality_check >> generate_report
generate_report >> end_task