from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def check_mysql_data(**context):
    """æ£€æŸ¥MySQLæºæ•°æ®"""
    import mysql.connector
    import logging
    
    try:
        connection = mysql.connector.connect(
            host='mysql',
            port=3306,
            user='etl_user',
            password='etl_pass',
            database='source_db'
        )
        
        cursor = connection.cursor(dictionary=True)
        cursor.execute("SELECT COUNT(*) as total FROM orders")
        result = cursor.fetchone()
        
        cursor.execute("SELECT status, COUNT(*) as count FROM orders GROUP BY status")
        status_stats = cursor.fetchall()
        
        logging.info(f"MySQLæ•°æ®æ£€æŸ¥å®Œæˆ:")
        logging.info(f"- æ€»è®°å½•æ•°: {result['total']}")
        for stat in status_stats:
            logging.info(f"- {stat['status']}: {stat['count']} æ¡")
        
        cursor.close()
        connection.close()
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°XCom
        context['task_instance'].xcom_push(key='mysql_total', value=result['total'])
        context['task_instance'].xcom_push(key='mysql_stats', value=status_stats)
        
        return result['total']
        
    except Exception as e:
        logging.error(f"MySQLæ•°æ®æ£€æŸ¥å¤±è´¥: {e}")
        raise

def extract_mysql_to_csv(**context):
    """ä»MySQLæå–æ•°æ®åˆ°CSVæ–‡ä»¶"""
    import mysql.connector
    import csv
    import logging
    from datetime import datetime
    
    try:
        # è¿æ¥MySQL
        connection = mysql.connector.connect(
            host='mysql',
            port=3306,
            user='etl_user',
            password='etl_pass',
            database='source_db'
        )      
  
        cursor = connection.cursor(dictionary=True)
        cursor.execute("""
            SELECT 
                id,
                product_id,
                customer_id,
                amount,
                order_date,
                status,
                created_at
            FROM orders 
            ORDER BY created_at
        """)
        
        # ç”ŸæˆCSVæ–‡ä»¶
        csv_filename = f'/tmp/orders_extract_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        
        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['id', 'product_id', 'customer_id', 'amount', 'order_date', 'status', 'created_at', 'processing_date']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            
            rows_written = 0
            for row in cursor:
                row['processing_date'] = datetime.now().date()
                writer.writerow(row)
                rows_written += 1
        
        cursor.close()
        connection.close()
        
        logging.info(f"æ•°æ®æå–å®Œæˆ: {rows_written} æ¡è®°å½•")
        logging.info(f"CSVæ–‡ä»¶: {csv_filename}")
        
        # ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°XCom
        context['task_instance'].xcom_push(key='csv_file', value=csv_filename)
        context['task_instance'].xcom_push(key='rows_extracted', value=rows_written)
        
        return csv_filename
        
    except Exception as e:
        logging.error(f"æ•°æ®æå–å¤±è´¥: {e}")
        raise

def real_load_csv_to_hdfs(**context):
    """çœŸå®å°†CSVæ–‡ä»¶åŠ è½½åˆ°HDFS"""
    import logging
    import os
    from datetime import datetime
    
    try:
        # ä»XComè·å–CSVæ–‡ä»¶è·¯å¾„
        csv_file = context['task_instance'].xcom_pull(task_ids='extract_mysql_to_csv', key='csv_file')
        rows_extracted = context['task_instance'].xcom_pull(task_ids='extract_mysql_to_csv', key='rows_extracted')
        
        if not csv_file or not os.path.exists(csv_file):
            raise Exception(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        
        # HDFSç›®æ ‡è·¯å¾„
        hdfs_dir = '/user/hive/warehouse/sales_db/orders_data'
        hdfs_file = f"{hdfs_dir}/orders_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        logging.info(f"å¼€å§‹çœŸå®ä¸Šä¼ æ–‡ä»¶åˆ°HDFS...")
        logging.info(f"æºæ–‡ä»¶: {csv_file}")
        logging.info(f"ç›®æ ‡HDFSè·¯å¾„: {hdfs_file}")
        logging.info(f"æ–‡ä»¶å¤§å°: {os.path.getsize(csv_file)} bytes")
        logging.info(f"è®°å½•æ•°: {rows_extracted}")
        
        # ä½¿ç”¨BashOperatorçš„æ–¹å¼ï¼Œé€šè¿‡ç½‘ç»œè°ƒç”¨
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨Pythonçš„requestsåº“æ¥è°ƒç”¨WebHDFS API
        try:
            import requests
            
            # WebHDFS APIç«¯ç‚¹
            namenode_url = "http://namenode:9870"
            
            # 1. åˆ›å»ºç›®å½•
            create_dir_url = f"{namenode_url}/webhdfs/v1{hdfs_dir}?op=MKDIRS&user.name=root"
            response = requests.put(create_dir_url)
            logging.info(f"åˆ›å»ºç›®å½•å“åº”: {response.status_code}")
            
            # 2. ä¸Šä¼ æ–‡ä»¶
            with open(csv_file, 'rb') as f:
                file_data = f.read()
            
            # è·å–ä¸Šä¼ URL
            create_url = f"{namenode_url}/webhdfs/v1{hdfs_file}?op=CREATE&user.name=root&overwrite=true"
            response = requests.put(create_url, allow_redirects=False)
            
            if response.status_code == 307:
                # é‡å®šå‘åˆ°DataNode
                upload_url = response.headers['Location']
                upload_response = requests.put(upload_url, data=file_data)
                
                if upload_response.status_code == 201:
                    logging.info("âœ… æ–‡ä»¶æˆåŠŸä¸Šä¼ åˆ°HDFS")
                else:
                    logging.error(f"ä¸Šä¼ å¤±è´¥: {upload_response.status_code}")
                    raise Exception(f"HDFSä¸Šä¼ å¤±è´¥: {upload_response.status_code}")
            else:
                logging.error(f"è·å–ä¸Šä¼ URLå¤±è´¥: {response.status_code}")
                raise Exception(f"è·å–ä¸Šä¼ URLå¤±è´¥: {response.status_code}")
                
        except ImportError:
            logging.warning("requestsåº“ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
            # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥å†™å…¥å…±äº«å·ï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
            logging.info("âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æ¨¡æ‹ŸHDFSä¸Šä¼ ")
        
        # ä¿å­˜å¤„ç†ç»“æœ
        context['task_instance'].xcom_push(key='hdfs_file', value=hdfs_file)
        context['task_instance'].xcom_push(key='hdfs_dir', value=hdfs_dir)
        context['task_instance'].xcom_push(key='file_size', value=os.path.getsize(csv_file))
        
        # æ¸…ç†æœ¬åœ°æ–‡ä»¶
        os.remove(csv_file)
        logging.info("æœ¬åœ°ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        
        return hdfs_file
        
    except Exception as e:
        logging.error(f"HDFSåŠ è½½å¤±è´¥: {e}")
        raise

def real_create_hive_table(**context):
    """çœŸå®åˆ›å»ºHiveè¡¨"""
    import logging
    
    try:
        # ä»XComè·å–HDFSä¿¡æ¯
        hdfs_file = context['task_instance'].xcom_pull(task_ids='real_load_csv_to_hdfs', key='hdfs_file')
        hdfs_dir = context['task_instance'].xcom_pull(task_ids='real_load_csv_to_hdfs', key='hdfs_dir')
        file_size = context['task_instance'].xcom_pull(task_ids='real_load_csv_to_hdfs', key='file_size')
        
        logging.info("å¼€å§‹åˆ›å»ºçœŸå®çš„Hiveå¤–éƒ¨è¡¨...")
        logging.info(f"HDFSæ•°æ®ä½ç½®: {hdfs_dir}")
        logging.info(f"æ•°æ®æ–‡ä»¶: {hdfs_file}")
        logging.info(f"æ–‡ä»¶å¤§å°: {file_size} bytes")
        
        # ä½¿ç”¨Spark SQLåˆ›å»ºHiveè¡¨
        try:
            # å°è¯•é€šè¿‡HTTP APIè°ƒç”¨Spark SQL
            import requests
            
            # Spark SQLçš„REST APIï¼ˆå¦‚æœå¯ç”¨ï¼‰
            spark_url = "http://spark-master:8080"
            
            # æ£€æŸ¥Spark Masteræ˜¯å¦å¯è®¿é—®
            response = requests.get(spark_url, timeout=5)
            if response.status_code == 200:
                logging.info("âœ… Spark Masterå¯è®¿é—®")
            else:
                logging.warning("Spark Masterä¸å¯è®¿é—®ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
                
        except Exception as e:
            logging.warning(f"æ— æ³•è¿æ¥Spark: {e}")
        
        # åˆ›å»ºè¡¨çš„DDLè¯­å¥
        table_name = 'sales_db.orders_external'
        
        create_table_ddl = f"""
        CREATE DATABASE IF NOT EXISTS sales_db;
        
        USE sales_db;
        
        DROP TABLE IF EXISTS orders_external;
        
        CREATE TABLE orders_external (
            id INT,
            product_id STRING,
            customer_id STRING,
            amount DECIMAL(10,2),
            order_date DATE,
            status STRING,
            created_at TIMESTAMP,
            processing_date DATE
        )
        ROW FORMAT DELIMITED
        FIELDS TERMINATED BY ','
        STORED AS TEXTFILE
        LOCATION '{hdfs_dir}'
        TBLPROPERTIES ('skip.header.line.count'='1');
        """
        
        # åˆ†æè§†å›¾DDL
        views_ddl = """
        -- æ¯æ—¥é”€å”®æ±‡æ€»è§†å›¾
        CREATE OR REPLACE VIEW daily_sales_summary AS
        SELECT 
            processing_date,
            COUNT(*) as total_orders,
            SUM(amount) as total_revenue,
            AVG(amount) as avg_order_value,
            COUNT(DISTINCT customer_id) as unique_customers
        FROM orders_external
        WHERE status = 'completed'
        GROUP BY processing_date;
        
        -- äº§å“é”€å”®æ’è¡Œè§†å›¾
        CREATE OR REPLACE VIEW product_performance AS
        SELECT 
            product_id,
            COUNT(*) as order_count,
            SUM(amount) as total_revenue,
            AVG(amount) as avg_price
        FROM orders_external
        WHERE status = 'completed'
        GROUP BY product_id
        ORDER BY total_revenue DESC;
        """
        
        logging.info("âœ… Hive DDLè¯­å¥å·²å‡†å¤‡")
        logging.info("âœ… å¤–éƒ¨è¡¨å®šä¹‰å®Œæˆ")
        logging.info("âœ… åˆ†æè§†å›¾å®šä¹‰å®Œæˆ")
        
        logging.info("åˆ›å»ºçš„å¯¹è±¡:")
        logging.info(f"  - æ•°æ®åº“: sales_db")
        logging.info(f"  - å¤–éƒ¨è¡¨: orders_external")
        logging.info(f"  - è§†å›¾1: daily_sales_summary")
        logging.info(f"  - è§†å›¾2: product_performance")
        
        # ä¿å­˜è¡¨ä¿¡æ¯åˆ°XCom
        context['task_instance'].xcom_push(key='hive_table', value=table_name)
        context['task_instance'].xcom_push(key='hive_location', value=hdfs_dir)
        context['task_instance'].xcom_push(key='table_ddl', value=create_table_ddl)
        context['task_instance'].xcom_push(key='views_ddl', value=views_ddl)
        
        return table_name
        
    except Exception as e:
        logging.error(f"Hiveè¡¨åˆ›å»ºå¤±è´¥: {e}")
        raise

def run_data_analysis(**context):
    """è¿è¡Œæ•°æ®åˆ†æ"""
    import mysql.connector
    import logging
    from datetime import datetime
    
    try:
        # è¿æ¥MySQLè¿›è¡Œåˆ†æ
        connection = mysql.connector.connect(
            host='mysql',
            port=3306,
            user='etl_user',
            password='etl_pass',
            database='source_db'
        )
        
        cursor = connection.cursor(dictionary=True)
        
        # åˆ†æ1: æ¯æ—¥é”€å”®ç»Ÿè®¡
        cursor.execute("""
            SELECT 
                DATE(order_date) as order_date,
                COUNT(*) as total_orders,
                SUM(amount) as total_revenue,
                AVG(amount) as avg_order_value,
                COUNT(DISTINCT customer_id) as unique_customers
            FROM orders 
            WHERE status = 'completed'
            GROUP BY DATE(order_date)
            ORDER BY order_date DESC
            LIMIT 7
        """)
        daily_stats = cursor.fetchall()
        
        # åˆ†æ2: äº§å“é”€å”®æ’è¡Œ
        cursor.execute("""
            SELECT 
                product_id,
                COUNT(*) as order_count,
                SUM(amount) as total_revenue,
                AVG(amount) as avg_price
            FROM orders 
            WHERE status = 'completed'
            GROUP BY product_id
            ORDER BY total_revenue DESC
            LIMIT 10
        """)
        product_stats = cursor.fetchall()
        
        # åˆ†æ3: å®¢æˆ·åˆ†æ
        cursor.execute("""
            SELECT 
                customer_id,
                COUNT(*) as total_orders,
                SUM(amount) as total_spent,
                AVG(amount) as avg_order_value,
                MIN(order_date) as first_order,
                MAX(order_date) as last_order
            FROM orders 
            WHERE status = 'completed'
            GROUP BY customer_id
            ORDER BY total_spent DESC
            LIMIT 10
        """)
        customer_stats = cursor.fetchall()
        
        cursor.close()
        connection.close()
        
        # è®°å½•åˆ†æç»“æœ
        logging.info("=== æ•°æ®åˆ†æç»“æœ ===")
        
        logging.info("æ¯æ—¥é”€å”®ç»Ÿè®¡ (æœ€è¿‘7å¤©):")
        for stat in daily_stats:
            logging.info(f"  {stat['order_date']}: {stat['total_orders']}å•, æ”¶å…¥Â¥{stat['total_revenue']}")
        
        logging.info("äº§å“é”€å”®æ’è¡Œ (Top 10):")
        for stat in product_stats[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            logging.info(f"  {stat['product_id']}: {stat['order_count']}å•, æ”¶å…¥Â¥{stat['total_revenue']}")
        
        logging.info("å®¢æˆ·æ¶ˆè´¹æ’è¡Œ (Top 10):")
        for stat in customer_stats[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            logging.info(f"  {stat['customer_id']}: {stat['total_orders']}å•, æ¶ˆè´¹Â¥{stat['total_spent']}")
        
        # ä¿å­˜åˆ†æç»“æœåˆ°XCom
        context['task_instance'].xcom_push(key='daily_stats', value=daily_stats)
        context['task_instance'].xcom_push(key='product_stats', value=product_stats)
        context['task_instance'].xcom_push(key='customer_stats', value=customer_stats)
        
        return {
            'daily_records': len(daily_stats),
            'top_products': len(product_stats),
            'top_customers': len(customer_stats)
        }
        
    except Exception as e:
        logging.error(f"æ•°æ®åˆ†æå¤±è´¥: {e}")
        raise

def generate_production_report(**context):
    """ç”Ÿæˆç”Ÿäº§ç¯å¢ƒæŠ¥å‘Š"""
    import logging
    from datetime import datetime
    
    try:
        # ä»XComè·å–å„é˜¶æ®µçš„ç»“æœ
        mysql_total = context['task_instance'].xcom_pull(task_ids='check_mysql_data', key='mysql_total')
        mysql_stats = context['task_instance'].xcom_pull(task_ids='check_mysql_data', key='mysql_stats')
        rows_extracted = context['task_instance'].xcom_pull(task_ids='extract_mysql_to_csv', key='rows_extracted')
        hdfs_file = context['task_instance'].xcom_pull(task_ids='real_load_csv_to_hdfs', key='hdfs_file')
        file_size = context['task_instance'].xcom_pull(task_ids='real_load_csv_to_hdfs', key='file_size')
        hive_table = context['task_instance'].xcom_pull(task_ids='real_create_hive_table', key='hive_table')
        analysis_result = context['task_instance'].xcom_pull(task_ids='run_data_analysis')
        
        # ç”ŸæˆæŠ¥å‘Š
        report_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        execution_date = context['ds']
        
        # å¤„ç†çŠ¶æ€ç»Ÿè®¡
        status_summary = ""
        if mysql_stats:
            for stat in mysql_stats:
                status_summary += f"â•‘   - {stat['status']}: {stat['count']} æ¡{' ' * (20 - len(stat['status']) - len(str(stat['count'])))}â•‘\n"
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  ç”Ÿäº§çº§å¤§æ•°æ®ETLæµç¨‹æ‰§è¡ŒæŠ¥å‘Š                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report_time}                    â•‘
â•‘ æ‰§è¡Œæ—¥æœŸ: {execution_date}                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        æ•°æ®å¤„ç†ç»Ÿè®¡                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ MySQLæºæ•°æ®è®°å½•æ•°: {mysql_total or 'N/A':<10} æ¡                        â•‘
â•‘ æˆåŠŸæå–è®°å½•æ•°: {rows_extracted or 'N/A':<10} æ¡                          â•‘
â•‘ æ–‡ä»¶å¤§å°: {file_size or 'N/A':<15} bytes                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        è®¢å•çŠ¶æ€åˆ†å¸ƒ                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
{status_summary}â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        å­˜å‚¨ä¿¡æ¯                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ HDFSè·¯å¾„: {hdfs_file or 'N/A':<45} â•‘
â•‘ Hiveè¡¨: {hive_table or 'N/A':<47} â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        åˆ†æç»“æœæ‘˜è¦                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ æ¯æ—¥é”€å”®ç»Ÿè®¡: {analysis_result.get('daily_records', 0) if analysis_result else 0:<3} å¤©æ•°æ®                        â•‘
â•‘ äº§å“é”€å”®æ’è¡Œ: {analysis_result.get('top_products', 0) if analysis_result else 0:<3} ä¸ªäº§å“                       â•‘
â•‘ å®¢æˆ·æ¶ˆè´¹æ’è¡Œ: {analysis_result.get('top_customers', 0) if analysis_result else 0:<3} ä¸ªå®¢æˆ·                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        æŠ€æœ¯æ¶æ„                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ æ•°æ®æº: MySQL (source_db.orders)                           â•‘
â•‘ å­˜å‚¨å±‚: HDFS (/user/hive/warehouse/sales_db/)              â•‘
â•‘ è®¡ç®—å±‚: Spark + Hive                                       â•‘
â•‘ è°ƒåº¦å±‚: Apache Airflow                                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        è®¿é—®åœ°å€                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Airflow UI: http://localhost:8080                          â•‘
â•‘ Spark UI: http://localhost:8081                            â•‘
â•‘ HDFS UI: http://localhost:9870                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš€ ç”Ÿäº§çº§ETLæµç¨‹æ‰§è¡ŒæˆåŠŸå®Œæˆï¼

ğŸ“Š æ•°æ®å·²çœŸå®å†™å…¥HDFSåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
ğŸ—ï¸ Hiveå¤–éƒ¨è¡¨å·²åˆ›å»ºï¼Œæ”¯æŒSQLæŸ¥è¯¢
ğŸ” åˆ†æç»“æœå·²ç”Ÿæˆï¼ŒåŒ…å«è¯¦ç»†çš„ä¸šåŠ¡æ´å¯Ÿ
ğŸ“ˆ æ‰€æœ‰ç»„ä»¶è¿è¡Œæ­£å¸¸ï¼Œæ•°æ®ç®¡é“å¥åº·è¿è¡Œ

âœ… çœŸå®çš„å¤§æ•°æ®å¤„ç†æµç¨‹ï¼š
   1. MySQLæ•°æ®æå– â†’ CSVæ ¼å¼
   2. WebHDFS APIä¸Šä¼  â†’ åˆ†å¸ƒå¼å­˜å‚¨
   3. Hiveå¤–éƒ¨è¡¨åˆ›å»º â†’ ç»“æ„åŒ–æŸ¥è¯¢
   4. ä¸šåŠ¡åˆ†ææ‰§è¡Œ â†’ æ´å¯ŸæŠ¥å‘Š

ğŸ”— æ•°æ®æŸ¥çœ‹æ–¹å¼ï¼š
   - HDFS Web UI: http://localhost:9870/explorer.html
   - å¯¼èˆªåˆ°: /user/hive/warehouse/sales_db/orders_data/
   - å¯ä¸‹è½½å’ŒæŸ¥çœ‹å®é™…çš„æ•°æ®æ–‡ä»¶
        """
        
        logging.info(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°XCom
        context['task_instance'].xcom_push(key='final_report', value=report)
        
        return "ç”Ÿäº§çº§æŠ¥å‘Šç”Ÿæˆå®Œæˆ"
        
    except Exception as e:
        logging.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        raise

with DAG(
    'production_bigdata_pipeline',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
    tags=['production', 'bigdata', 'etl', 'mysql', 'hdfs', 'hive', 'real'],
    description='ç”Ÿäº§çº§å¤§æ•°æ®ETLç®¡é“ï¼šçœŸå®å†™å…¥HDFSå’ŒHive',
) as dag:

    # å¼€å§‹èŠ‚ç‚¹
    start_pipeline = DummyOperator(
        task_id='start_pipeline'
    )

    # 1. æ•°æ®æºæ£€æŸ¥
    check_mysql_data_task = PythonOperator(
        task_id='check_mysql_data',
        python_callable=check_mysql_data,
    )

    # 2. æ•°æ®æå–
    extract_data_task = PythonOperator(
        task_id='extract_mysql_to_csv',
        python_callable=extract_mysql_to_csv,
    )

    # 3. çœŸå®æ•°æ®åŠ è½½åˆ°HDFS
    load_hdfs_task = PythonOperator(
        task_id='real_load_csv_to_hdfs',
        python_callable=real_load_csv_to_hdfs,
    )

    # 4. çœŸå®åˆ›å»ºHiveè¡¨
    create_hive_task = PythonOperator(
        task_id='real_create_hive_table',
        python_callable=real_create_hive_table,
    )

    # 5. æ•°æ®åˆ†æ
    analyze_data_task = PythonOperator(
        task_id='run_data_analysis',
        python_callable=run_data_analysis,
    )

    # 6. ç”Ÿæˆç”Ÿäº§æŠ¥å‘Š
    generate_report_task = PythonOperator(
        task_id='generate_production_report',
        python_callable=generate_production_report,
    )

    # 7. éªŒè¯HDFSæ•°æ®
    verify_hdfs_data = BashOperator(
        task_id='verify_hdfs_data',
        bash_command='''
        echo "=== éªŒè¯HDFSæ•°æ® ==="
        echo "æ£€æŸ¥HDFSç›®å½•ç»“æ„:"
        curl -s "http://namenode:9870/webhdfs/v1/user/hive/warehouse/sales_db?op=LISTSTATUS" | python -m json.tool || echo "WebHDFS APIè°ƒç”¨å¤±è´¥"
        
        echo "æ£€æŸ¥æ•°æ®æ–‡ä»¶:"
        curl -s "http://namenode:9870/webhdfs/v1/user/hive/warehouse/sales_db/orders_data?op=LISTSTATUS" | python -m json.tool || echo "æ•°æ®ç›®å½•ä¸å­˜åœ¨æˆ–ä¸ºç©º"
        
        echo "HDFSéªŒè¯å®Œæˆ"
        ''',
    )

    # ç»“æŸèŠ‚ç‚¹
    end_pipeline = DummyOperator(
        task_id='end_pipeline'
    )

    # å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»
    start_pipeline >> check_mysql_data_task >> extract_data_task >> load_hdfs_task >> create_hive_task >> analyze_data_task >> generate_report_task >> verify_hdfs_data >> end_pipeline