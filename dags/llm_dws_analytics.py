"""
LLM - Airflow DAG Version
ä½¿ç”¨Agno + DeepSeekå¤§æ¨¡å‹åˆ†ææ¯æ—¥è®¢å•æ˜ç»†KPIæ•°æ®çš„Airflow DAG
æ”¯æŒAgno API (ä¼˜å…ˆ) å’Œ DeepSeek API (å¤‡é€‰)
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.bash import BashOperator
import pandas as pd

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': False,
    'email_on_retry': False,
}

def analyze_daily_kpi_with_llm(**context):
    """ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ¯æ—¥KPIæ•°æ®"""
    import os
    import sys
    import json
    import logging
    from datetime import datetime, timedelta
    from typing import Dict, List, Any, Optional
    import pandas as pd
    from pyspark.sql import SparkSession
    import tempfile
    
    spark = None
    try:
        # åˆ›å»ºSparkä¼šè¯ - ä½¿ç”¨ä¸å…¶ä»–DAGç›¸åŒçš„é…ç½®
        spark = SparkSession.builder \
            .appName("Daily KPI LLM Analysis") \
            .master("local[1]") \
            .config("spark.sql.catalogImplementation", "hive") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
            .config("spark.hadoop.hive.metastore.uris", "thrift://hive-metastore:9083") \
            .config("spark.sql.warehouse.dir", "hdfs://namenode:9000/user/hive/warehouse") \
            .config("spark.driver.memory", "1g") \
            .config("spark.executor.memory", "1g") \
            .config("spark.ui.enabled", "false") \
            .config("spark.driver.extraJavaOptions", "-XX:+UseG1GC") \
            .config("spark.executor.extraJavaOptions", "-XX:+UseG1GC") \
            .enableHiveSupport() \
            .getOrCreate()
        
        logging.info("âœ… Sparkä¼šè¯åˆ›å»ºæˆåŠŸ")
        
        # æ£€æŸ¥DWSæ•°æ®åº“å’Œè§†å›¾æ˜¯å¦å­˜åœ¨
        try:
            databases = spark.sql("SHOW DATABASES").collect()
            db_names = [db[0] for db in databases]
            logging.info(f"å¯ç”¨æ•°æ®åº“: {db_names}")
            
            target_db = 'dws_db' if 'dws_db' in db_names else 'default'
            logging.info(f"ä½¿ç”¨æ•°æ®åº“: {target_db}")
            
            spark.sql(f"USE {target_db}")
            
            # æ£€æŸ¥è§†å›¾æ˜¯å¦å­˜åœ¨
            views = spark.sql("SHOW VIEWS").collect()
            view_names = [view[1] for view in views]
            logging.info(f"{target_db}ä¸­çš„è§†å›¾: {view_names}")
            
            if 'dws_orderdetails_daily_kpi' not in view_names:
                logging.error(f"âŒ åœ¨ {target_db} æ•°æ®åº“ä¸­æœªæ‰¾åˆ° dws_orderdetails_daily_kpi è§†å›¾")
                raise Exception(f"dws_orderdetails_daily_kpi è§†å›¾ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ dws_orderdetails_analytics DAG")
            
        except Exception as e:
            logging.error(f"âŒ DWSè§†å›¾æ£€æŸ¥å¤±è´¥: {e}")
            raise Exception("DWSè§†å›¾ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ dws_orderdetails_analytics DAG")
        
        # è·å–åˆ†æå¤©æ•°å‚æ•°
        days = context.get('params', {}).get('analysis_days', 7)
        logging.info(f"ğŸ“Š è·å–æœ€è¿‘ {days} å¤©çš„KPIæ•°æ®...")
        
        # æŸ¥è¯¢KPIæ•°æ®
        query = f'''
        SELECT 
            order_date,
            total_items,
            total_amount,
            avg_item_value,
            delivery_rate,
            cancellation_rate,
            discount_rate,
            high_value_rate,
            performance_grade
        FROM dws_orderdetails_daily_kpi
        WHERE order_date >= date_sub(current_date(), {days})
        ORDER BY order_date DESC
        '''

        df = spark.sql(query)
        pandas_df = df.toPandas()
        
        if pandas_df.empty:
            logging.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„KPIæ•°æ®")
            context['task_instance'].xcom_push(key='analysis_status', value='no_data')
            return "æ²¡æœ‰å¯ç”¨çš„KPIæ•°æ®è¿›è¡Œåˆ†æ"
        
        logging.info(f"âœ… è·å–åˆ° {len(pandas_df)} å¤©çš„KPIæ•°æ®")
        
        # å‡†å¤‡åˆ†æä¸Šä¸‹æ–‡
        logging.info("ğŸ“ å‡†å¤‡åˆ†æä¸Šä¸‹æ–‡...")
        context_data = prepare_analysis_context(pandas_df)
        
        # è°ƒç”¨LLMåˆ†æ
        logging.info("ğŸ¤– è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ†æ...")
        model = context.get('params', {}).get('model', 'deepseek-chat')
        analysis = call_llm_analysis(context_data, model)
        
        # ä¿å­˜åˆ†ææŠ¥å‘Š
        logging.info("ğŸ’¾ ä¿å­˜åˆ†ææŠ¥å‘Š...")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_dir = tempfile.mkdtemp(prefix='llm_analysis_')
        report_file = os.path.join(temp_dir, f'daily_kpi_analysis_{timestamp}.md')
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# è®¢å•æ˜ç»†æ¯æ—¥KPIåˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**åˆ†æå¤©æ•°**: {days} å¤©\n")
            f.write(f"**ä½¿ç”¨æ¨¡å‹**: {model}\n\n")
            f.write(analysis)
        
        logging.info(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # ä¿å­˜åˆ°XCom
        context['task_instance'].xcom_push(key='analysis_report_file', value=report_file)
        context['task_instance'].xcom_push(key='analysis_temp_dir', value=temp_dir)
        context['task_instance'].xcom_push(key='analysis_status', value='success')
        context['task_instance'].xcom_push(key='analysis_summary', value={
            'days_analyzed': days,
            'records_count': len(pandas_df),
            'model_used': model,
            'report_file': report_file
        })
        
        logging.info("âœ… LLMåˆ†æå®Œæˆ!")
        return report_file
        
    except Exception as e:
        logging.error(f"âŒ LLMåˆ†æå¤±è´¥: {e}")
        context['task_instance'].xcom_push(key='analysis_status', value='failed')
        raise
    finally:
        if spark:
            try:
                spark.stop()
                logging.info("Sparkä¼šè¯å·²å…³é—­")
            except Exception as e:
                logging.warning(f"å…³é—­Sparkä¼šè¯æ—¶å‡ºç°è­¦å‘Š: {e}")

def prepare_analysis_context(df: pd.DataFrame) -> str:
    """å‡†å¤‡åˆ†æä¸Šä¸‹æ–‡æ•°æ®"""
    if df.empty:
        return "æ²¡æœ‰å¯ç”¨çš„KPIæ•°æ®è¿›è¡Œåˆ†æã€‚"
    
    # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "æ•°æ®æ¦‚è§ˆ": {
            "åˆ†æå¤©æ•°": len(df),
            "æ—¥æœŸèŒƒå›´": f"{df['order_date'].min()} åˆ° {df['order_date'].max()}",
            "æ€»è®¢å•æ˜ç»†æ•°": int(df['total_items'].sum()),
            "æ€»äº¤æ˜“é‡‘é¢": float(df['total_amount'].sum())
        },
        "å…³é”®æŒ‡æ ‡ç»Ÿè®¡": {
            "å¹³å‡æ¯æ—¥è®¢å•æ˜ç»†æ•°": float(df['total_items'].mean()),
            "å¹³å‡æ¯æ—¥äº¤æ˜“é‡‘é¢": float(df['total_amount'].mean()),
            "å¹³å‡å•é¡¹ä»·å€¼": float(df['avg_item_value'].mean()),
            "å¹³å‡äº¤ä»˜ç‡": f"{df['delivery_rate'].mean():.1f}%",
            "å¹³å‡å–æ¶ˆç‡": f"{df['cancellation_rate'].mean():.1f}%",
            "å¹³å‡æŠ˜æ‰£ç‡": f"{df['discount_rate'].mean():.1f}%",
            "å¹³å‡é«˜ä»·å€¼è®¢å•ç‡": f"{df['high_value_rate'].mean():.1f}%"
        },
        "ç»©æ•ˆç­‰çº§åˆ†å¸ƒ": df['performance_grade'].value_counts().to_dict(),
        "è¶‹åŠ¿åˆ†æ": {
            "è®¢å•æ˜ç»†æ•°è¶‹åŠ¿": "å¢é•¿" if df['total_items'].iloc[0] > df['total_items'].iloc[-1] else "ä¸‹é™",
            "äº¤æ˜“é‡‘é¢è¶‹åŠ¿": "å¢é•¿" if df['total_amount'].iloc[0] > df['total_amount'].iloc[-1] else "ä¸‹é™",
            "äº¤ä»˜ç‡è¶‹åŠ¿": "æ”¹å–„" if df['delivery_rate'].iloc[0] > df['delivery_rate'].iloc[-1] else "æ¶åŒ–"
        }
    }
    
    # æ„å»ºè¯¦ç»†çš„æ¯æ—¥æ•°æ®
    daily_details = []
    for _, row in df.iterrows():
        daily_details.append({
            "æ—¥æœŸ": row['order_date'],
            "è®¢å•æ˜ç»†æ•°": int(row['total_items']),
            "äº¤æ˜“é‡‘é¢": float(row['total_amount']),
            "å¹³å‡å•é¡¹ä»·å€¼": float(row['avg_item_value']),
            "äº¤ä»˜ç‡": f"{row['delivery_rate']:.1f}%",
            "å–æ¶ˆç‡": f"{row['cancellation_rate']:.1f}%",
            "æŠ˜æ‰£ç‡": f"{row['discount_rate']:.1f}%",
            "é«˜ä»·å€¼è®¢å•ç‡": f"{row['high_value_rate']:.1f}%",
            "ç»©æ•ˆç­‰çº§": row['performance_grade']
        })
    
    context = f'''
# è®¢å•æ˜ç»†æ¯æ—¥KPIåˆ†ææ•°æ®

## æ•°æ®æ¦‚è§ˆ
{json.dumps(stats['æ•°æ®æ¦‚è§ˆ'], ensure_ascii=False, indent=2)}

## å…³é”®æŒ‡æ ‡ç»Ÿè®¡
{json.dumps(stats['å…³é”®æŒ‡æ ‡ç»Ÿè®¡'], ensure_ascii=False, indent=2)}

## ç»©æ•ˆç­‰çº§åˆ†å¸ƒ
{json.dumps(stats['ç»©æ•ˆç­‰çº§åˆ†å¸ƒ'], ensure_ascii=False, indent=2)}

## è¶‹åŠ¿åˆ†æ
{json.dumps(stats['è¶‹åŠ¿åˆ†æ'], ensure_ascii=False, indent=2)}

## æ¯æ—¥è¯¦ç»†æ•°æ®
{json.dumps(daily_details, ensure_ascii=False, indent=2)}
'''
    
    return context

def call_llm_analysis(context: str, model: str = "gpt-4o-mini") -> str:
    """è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ†æ - æ”¯æŒAgnoå’ŒDeepSeek"""
    import logging
    import os
    import requests
    
    try:
        # æ„å»ºåˆ†ææç¤º
        prompt = f'''
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹è®¢å•æ˜ç»†æ¯æ—¥KPIæ•°æ®è¿›è¡Œæ·±å…¥åˆ†æï¼š

{context}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¿›è¡Œåˆ†æï¼š

1. **ä¸šåŠ¡å¥åº·åº¦è¯„ä¼°**
   - æ•´ä½“ä¸šåŠ¡è¡¨ç°å¦‚ä½•ï¼Ÿ
   - å…³é”®æŒ‡æ ‡æ˜¯å¦åœ¨å¥åº·èŒƒå›´å†…ï¼Ÿ
   - æœ‰å“ªäº›ç§¯ææˆ–æ¶ˆæçš„ä¿¡å·ï¼Ÿ

2. **è¶‹åŠ¿åˆ†æ**
   - å„é¡¹æŒ‡æ ‡çš„å˜åŒ–è¶‹åŠ¿å¦‚ä½•ï¼Ÿ
   - æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„å‘¨æœŸæ€§æˆ–å­£èŠ‚æ€§æ¨¡å¼ï¼Ÿ
   - å“ªäº›æŒ‡æ ‡éœ€è¦é‡ç‚¹å…³æ³¨ï¼Ÿ

3. **é—®é¢˜è¯†åˆ«**
   - å‘ç°äº†å“ªäº›æ½œåœ¨é—®é¢˜ï¼Ÿ
   - å“ªäº›æŒ‡æ ‡è¡¨ç°å¼‚å¸¸ï¼Ÿ
   - å¯èƒ½çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ

4. **æ”¹è¿›å»ºè®®**
   - é’ˆå¯¹å‘ç°çš„é—®é¢˜ï¼Œæœ‰ä»€ä¹ˆå…·ä½“çš„æ”¹è¿›å»ºè®®ï¼Ÿ
   - å¦‚ä½•æå‡å…³é”®æŒ‡æ ‡çš„è¡¨ç°ï¼Ÿ
   - æœ‰ä»€ä¹ˆé¢„é˜²æªæ–½å¯ä»¥é‡‡å–ï¼Ÿ

5. **é¢„è­¦å’Œç›‘æ§**
   - å“ªäº›æŒ‡æ ‡éœ€è¦è®¾ç½®é¢„è­¦é˜ˆå€¼ï¼Ÿ
   - å»ºè®®çš„ç›‘æ§é¢‘ç‡æ˜¯ä»€ä¹ˆï¼Ÿ
   - å¦‚ä½•å»ºç«‹æœ‰æ•ˆçš„ç›‘æ§ä½“ç³»ï¼Ÿ

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œåˆ†æè¦å…·ä½“ã€å®ç”¨ï¼Œå¹¶æä¾›å¯æ‰§è¡Œçš„å»ºè®®ã€‚
'''

        # ä¼˜å…ˆå°è¯•ä½¿ç”¨Agno API
        agno_api_key = os.getenv("AGNO_API_KEY")
        if agno_api_key and agno_api_key != "your-api-key":
            try:
                logging.info("ğŸ¤– ä½¿ç”¨Agno APIè¿›è¡Œåˆ†æ...")
                headers = {
                    "Authorization": f"Bearer {agno_api_key}",
                    "Content-Type": "application/json"
                }
                
                data = {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ç”µå•†å’Œè®¢å•æ•°æ®åˆ†æã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": 4000,
                    "temperature": 0.7
                }
                
                response = requests.post(
                    "https://api.agno.com/v1/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=60
                )
                
                if response.status_code == 200:
                    analysis = response.json()["choices"][0]["message"]["content"]
                    logging.info("âœ… Agnoåˆ†æå®Œæˆ")
                    return analysis
                else:
                    logging.warning(f"Agno APIè°ƒç”¨å¤±è´¥: {response.status_code}, å°è¯•DeepSeek")
                    
            except Exception as e:
                logging.warning(f"Agno APIè°ƒç”¨å¼‚å¸¸: {e}, å°è¯•DeepSeek")
        
        # å¤‡é€‰ï¼šä½¿ç”¨DeepSeek API
        deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
        if deepseek_api_key and deepseek_api_key != "your-api-key":
            try:
                logging.info("ğŸ¤– ä½¿ç”¨DeepSeek APIè¿›è¡Œåˆ†æ...")
                headers = {
                    "Authorization": f"Bearer {deepseek_api_key}",
                    "Content-Type": "application/json"
                }
                
                data = {
                    "model": "deepseek-chat",
                    "messages": [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ç”µå•†å’Œè®¢å•æ•°æ®åˆ†æã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": 4000,
                    "temperature": 0.7
                }
                
                response = requests.post(
                    "https://api.deepseek.com/v1/chat/completions",
                    headers=headers,
                    json=data,
                    timeout=60
                )
                
                if response.status_code == 200:
                    analysis = response.json()["choices"][0]["message"]["content"]
                    logging.info("âœ… DeepSeekåˆ†æå®Œæˆ")
                    return analysis
                else:
                    raise Exception(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                    
            except Exception as e:
                logging.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {e}")
        
        # å¦‚æœæ‰€æœ‰APIéƒ½ä¸å¯ç”¨ï¼Œè¿”å›åŸºç¡€åˆ†æ
        logging.warning("âš ï¸ æ‰€æœ‰LLM APIéƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
        return generate_basic_analysis(context)
        
    except Exception as e:
        logging.error(f"âŒ LLMåˆ†æå¤±è´¥: {e}")
        return generate_basic_analysis(context)

def generate_basic_analysis(context: str) -> str:
    """ç”ŸæˆåŸºç¡€åˆ†æï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰"""
    return f'''
# åŸºç¡€æ•°æ®åˆ†ææŠ¥å‘Š

## æ•°æ®æ¦‚è§ˆ
åŸºäºæä¾›çš„KPIæ•°æ®ï¼Œä»¥ä¸‹æ˜¯åŸºç¡€åˆ†æç»“æœï¼š

{context}

## åŸºç¡€å»ºè®®
1. **ç›‘æ§å…³é”®æŒ‡æ ‡**: é‡ç‚¹å…³æ³¨äº¤ä»˜ç‡å’Œå–æ¶ˆç‡çš„å˜åŒ–
2. **ä¼˜åŒ–æµç¨‹**: å¦‚æœå–æ¶ˆç‡è¾ƒé«˜ï¼Œéœ€è¦åˆ†æå–æ¶ˆåŸå› 
3. **æå‡æ•ˆç‡**: å…³æ³¨å¹³å‡å•é¡¹ä»·å€¼çš„å˜åŒ–è¶‹åŠ¿
4. **è´¨é‡æ§åˆ¶**: ç¡®ä¿ç»©æ•ˆç­‰çº§ä¿æŒåœ¨è‰¯å¥½æ°´å¹³

æ³¨æ„: è¿™æ˜¯åŸºç¡€åˆ†æç»“æœã€‚è¦è·å¾—æ›´æ·±å…¥çš„æ´å¯Ÿï¼Œè¯·é…ç½®LLM APIå¯†é’¥ã€‚
'''

def check_llm_dependencies(**context):
    """æ£€æŸ¥LLMåˆ†æä¾èµ–"""
    import logging
    import os
    
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        agno_api_key = os.getenv("AGNO_API_KEY")
        deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
        
        llm_available = False
        
        if agno_api_key and agno_api_key != "your-api-key":
            logging.info("âœ… AGNO_API_KEY å·²é…ç½®")
            llm_available = True
        elif deepseek_api_key and deepseek_api_key != "your-api-key":
            logging.info("âœ… DEEPSEEK_API_KEY å·²é…ç½®")
            llm_available = True
        else:
            logging.warning("âš ï¸ æœªè®¾ç½® AGNO_API_KEY æˆ– DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼Œå°†ä½¿ç”¨åŸºç¡€åˆ†æ")
            logging.info("è®¾ç½®æ–¹æ³•: export AGNO_API_KEY='your-api-key' æˆ– export DEEPSEEK_API_KEY='your-api-key'")
        
        context['task_instance'].xcom_push(key='llm_available', value=llm_available)
        
        # æ£€æŸ¥ä¾èµ–çš„DAGæ˜¯å¦å·²è¿è¡Œ
        from pyspark.sql import SparkSession
        
        spark = SparkSession.builder \
            .appName("Check LLM Dependencies") \
            .master("local[1]") \
            .config("spark.sql.catalogImplementation", "hive") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
            .config("spark.hadoop.hive.metastore.uris", "thrift://hive-metastore:9083") \
            .config("spark.driver.memory", "1g") \
            .config("spark.executor.memory", "1g") \
            .config("spark.ui.enabled", "false") \
            .enableHiveSupport() \
            .getOrCreate()
        
        try:
            databases = spark.sql("SHOW DATABASES").collect()
            db_names = [db[0] for db in databases]
            
            if 'dws_db' not in db_names:
                raise Exception("DWSæ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ dws_orderdetails_analytics DAG")
            
            spark.sql("USE dws_db")
            views = spark.sql("SHOW VIEWS").collect()
            view_names = [view[1] for view in views]
            
            if 'dws_orderdetails_daily_kpi' not in view_names:
                raise Exception("dws_orderdetails_daily_kpi è§†å›¾ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ dws_orderdetails_analytics DAG")
            
            logging.info("âœ… DWSä¾èµ–æ£€æŸ¥é€šè¿‡")
            context['task_instance'].xcom_push(key='dws_available', value=True)
            
        except Exception as e:
            logging.error(f"âŒ DWSä¾èµ–æ£€æŸ¥å¤±è´¥: {e}")
            context['task_instance'].xcom_push(key='dws_available', value=False)
            raise
        finally:
            spark.stop()
        
        return "dependencies_checked"
        
    except Exception as e:
        logging.error(f"ä¾èµ–æ£€æŸ¥å¤±è´¥: {e}")
        raise

def publish_analysis_report(**context):
    """å‘å¸ƒåˆ†ææŠ¥å‘Š"""
    import logging
    import os
    import shutil
    
    try:
        # è·å–åˆ†æç»“æœ
        analysis_status = context['task_instance'].xcom_pull(task_ids='analyze_daily_kpi_with_llm', key='analysis_status')
        
        if analysis_status != 'success':
            logging.warning(f"âš ï¸ åˆ†æçŠ¶æ€å¼‚å¸¸: {analysis_status}")
            return "analysis_failed"
        
        report_file = context['task_instance'].xcom_pull(task_ids='analyze_daily_kpi_with_llm', key='analysis_report_file')
        temp_dir = context['task_instance'].xcom_pull(task_ids='analyze_daily_kpi_with_llm', key='analysis_temp_dir')
        analysis_summary = context['task_instance'].xcom_pull(task_ids='analyze_daily_kpi_with_llm', key='analysis_summary')
        
        if not report_file or not os.path.exists(report_file):
            logging.error("âŒ åˆ†ææŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨")
            return "report_not_found"
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        reports_dir = "/opt/airflow/reports"
        os.makedirs(reports_dir, exist_ok=True)
        
        # å¤åˆ¶æŠ¥å‘Šåˆ°æ°¸ä¹…ä½ç½®
        final_report_path = os.path.join(reports_dir, os.path.basename(report_file))
        shutil.copy2(report_file, final_report_path)
        
        logging.info(f"âœ… åˆ†ææŠ¥å‘Šå·²å‘å¸ƒåˆ°: {final_report_path}")
        logging.info(f"ğŸ“Š åˆ†ææ‘˜è¦: {analysis_summary}")
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                logging.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
            except Exception as e:
                logging.warning(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
        
        # ä¿å­˜å‘å¸ƒä¿¡æ¯
        context['task_instance'].xcom_push(key='published_report_path', value=final_report_path)
        context['task_instance'].xcom_push(key='publish_status', value='success')
        
        return final_report_path
        
    except Exception as e:
        logging.error(f"å‘å¸ƒæŠ¥å‘Šå¤±è´¥: {e}")
        context['task_instance'].xcom_push(key='publish_status', value='failed')
        raise

# åˆ›å»ºDAG
with DAG(
    'llm_dws_analytics',
    default_args=default_args,
    schedule_interval='0 5 * * *',  # æ¯æ—¥å‡Œæ™¨5ç‚¹æ‰§è¡Œï¼ˆåœ¨dws_orderdetails_analyticsä¹‹åï¼‰
    catchup=False,
    max_active_runs=1,
    tags=['llm', 'analytics', 'dws', 'kpi', 'ai'],
    description='ä½¿ç”¨å¤§æ¨¡å‹åˆ†æDWSå±‚æ¯æ—¥KPIæ•°æ®',
    params={
        'analysis_days': 7,  # é»˜è®¤åˆ†ææœ€è¿‘7å¤©
        'model': 'gpt-4o-mini'  # é»˜è®¤ä½¿ç”¨çš„æ¨¡å‹ (Agnoæ”¯æŒ)
    }
) as dag:

    # å¼€å§‹ä»»åŠ¡
    start_task = DummyOperator(
        task_id='start_llm_analysis_pipeline',
        doc_md="""
        ## LLM DWS Analytics Pipeline å¼€å§‹
        
        è¿™ä¸ªæµç¨‹ä½¿ç”¨å¤§æ¨¡å‹åˆ†æDWSå±‚çš„æ¯æ—¥KPIæ•°æ®:
        1. æ£€æŸ¥LLMå’ŒDWSä¾èµ–
        2. ä»DWSå±‚è·å–KPIæ•°æ®
        3. ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ
        4. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        5. å‘å¸ƒæŠ¥å‘Šåˆ°æŒ‡å®šä½ç½®
        """
    )

    # 1. ä¾èµ–æ£€æŸ¥
    check_deps_task = PythonOperator(
        task_id='check_llm_dependencies',
        python_callable=check_llm_dependencies,
        doc_md="æ£€æŸ¥LLM APIé…ç½®å’ŒDWSå±‚æ•°æ®ä¾èµ–"
    )

    # 2. LLMåˆ†æ
    llm_analysis_task = PythonOperator(
        task_id='analyze_daily_kpi_with_llm',
        python_callable=analyze_daily_kpi_with_llm,
        doc_md="ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ¯æ—¥KPIæ•°æ®ï¼Œç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š"
    )

    # 3. å‘å¸ƒæŠ¥å‘Š
    publish_task = PythonOperator(
        task_id='publish_analysis_report',
        python_callable=publish_analysis_report,
        doc_md="å‘å¸ƒåˆ†ææŠ¥å‘Šåˆ°æŒ‡å®šä½ç½®å¹¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶"
    )

    # 4. éªŒè¯æŠ¥å‘Š
    verify_report_task = BashOperator(
        task_id='verify_analysis_report',
        bash_command='''
        echo "=== éªŒè¯LLMåˆ†ææŠ¥å‘Š ==="
        REPORTS_DIR="/opt/airflow/reports"
        
        if [ -d "$REPORTS_DIR" ]; then
            echo "æŠ¥å‘Šç›®å½•å­˜åœ¨: $REPORTS_DIR"
            echo "æœ€æ–°æŠ¥å‘Šæ–‡ä»¶:"
            ls -la "$REPORTS_DIR"/daily_kpi_analysis_*.md | tail -5
            
            # æ£€æŸ¥æœ€æ–°æŠ¥å‘Šå†…å®¹
            LATEST_REPORT=$(ls -t "$REPORTS_DIR"/daily_kpi_analysis_*.md | head -1)
            if [ -f "$LATEST_REPORT" ]; then
                echo "æœ€æ–°æŠ¥å‘Š: $LATEST_REPORT"
                echo "æŠ¥å‘Šå¤§å°: $(wc -c < "$LATEST_REPORT") å­—èŠ‚"
                echo "æŠ¥å‘Šè¡Œæ•°: $(wc -l < "$LATEST_REPORT") è¡Œ"
                echo "æŠ¥å‘Šé¢„è§ˆ:"
                head -20 "$LATEST_REPORT"
            else
                echo "âŒ æœªæ‰¾åˆ°æŠ¥å‘Šæ–‡ä»¶"
                exit 1
            fi
        else
            echo "âŒ æŠ¥å‘Šç›®å½•ä¸å­˜åœ¨"
            exit 1
        fi
        
        echo "âœ… LLMåˆ†ææŠ¥å‘ŠéªŒè¯å®Œæˆ"
        ''',
        doc_md="éªŒè¯ç”Ÿæˆçš„LLMåˆ†ææŠ¥å‘Š"
    )

    # ç»“æŸä»»åŠ¡
    end_task = DummyOperator(
        task_id='end_llm_analysis_pipeline',
        doc_md="LLM DWS Analytics Pipeline å®Œæˆ"
    )

    # å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»
    start_task >> check_deps_task >> llm_analysis_task >> publish_task >> verify_report_task >> end_task