from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def check_mysql_data(**context):
    """æ£€æŸ¥MySQLæºæ•°æ®"""
    import mysql.connector
    import logging
    
    try:
        connection = mysql.connector.connect(
            host='mysql',
            port=3306,
            user='etl_user',
            password='etl_pass',
            database='source_db'
        )
        
        cursor = connection.cursor(dictionary=True)
        cursor.execute("SELECT COUNT(*) as total FROM orders")
        result = cursor.fetchone()
        
        cursor.execute("SELECT status, COUNT(*) as count FROM orders GROUP BY status")
        status_stats = cursor.fetchall()
        
        logging.info(f"MySQLæ•°æ®æ£€æŸ¥å®Œæˆ:")
        logging.info(f"- æ€»è®°å½•æ•°: {result['total']}")
        for stat in status_stats:
            logging.info(f"- {stat['status']}: {stat['count']} æ¡")
        
        cursor.close()
        connection.close()
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°XCom
        context['task_instance'].xcom_push(key='mysql_total', value=result['total'])
        context['task_instance'].xcom_push(key='mysql_stats', value=status_stats)
        
        return result['total']
        
    except Exception as e:
        logging.error(f"MySQLæ•°æ®æ£€æŸ¥å¤±è´¥: {e}")
        raise

def extract_mysql_to_csv(**context):
    """ä»MySQLæå–æ•°æ®åˆ°CSVæ–‡ä»¶"""
    import mysql.connector
    import csv
    import logging
    from datetime import datetime
    
    try:
        # è¿æ¥MySQL
        connection = mysql.connector.connect(
            host='mysql',
            port=3306,
            user='etl_user',
            password='etl_pass',
            database='source_db'
        )
        
        cursor = connection.cursor(dictionary=True)
        cursor.execute("""
            SELECT 
                id,
                product_id,
                customer_id,
                amount,
                order_date,
                status,
                created_at
            FROM orders 
            ORDER BY created_at
        """)
        
        # ç”ŸæˆCSVæ–‡ä»¶
        csv_filename = f'/tmp/orders_extract_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        
        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['id', 'product_id', 'customer_id', 'amount', 'order_date', 'status', 'created_at', 'processing_date']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            
            rows_written = 0
            for row in cursor:
                row['processing_date'] = datetime.now().date()
                writer.writerow(row)
                rows_written += 1
        
        cursor.close()
        connection.close()
        
        logging.info(f"æ•°æ®æå–å®Œæˆ: {rows_written} æ¡è®°å½•")
        logging.info(f"CSVæ–‡ä»¶: {csv_filename}")
        
        # ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ°XCom
        context['task_instance'].xcom_push(key='csv_file', value=csv_filename)
        context['task_instance'].xcom_push(key='rows_extracted', value=rows_written)
        
        return csv_filename
        
    except Exception as e:
        logging.error(f"æ•°æ®æå–å¤±è´¥: {e}")
        raise

def load_csv_to_hdfs(**context):
    """å°†CSVæ–‡ä»¶åŠ è½½åˆ°HDFSï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    import logging
    import os
    from datetime import datetime
    
    try:
        # ä»XComè·å–CSVæ–‡ä»¶è·¯å¾„
        csv_file = context['task_instance'].xcom_pull(task_ids='extract_mysql_to_csv', key='csv_file')
        rows_extracted = context['task_instance'].xcom_pull(task_ids='extract_mysql_to_csv', key='rows_extracted')
        
        if not csv_file or not os.path.exists(csv_file):
            raise Exception(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        
        # æ¨¡æ‹ŸHDFSä¸Šä¼ è¿‡ç¨‹
        hdfs_dir = '/user/hive/warehouse/sales_db/orders_data'
        hdfs_file = f"{hdfs_dir}/orders_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        logging.info(f"å¼€å§‹æ¨¡æ‹ŸHDFSä¸Šä¼ è¿‡ç¨‹...")
        logging.info(f"æºæ–‡ä»¶: {csv_file}")
        logging.info(f"ç›®æ ‡HDFSè·¯å¾„: {hdfs_file}")
        logging.info(f"æ–‡ä»¶å¤§å°: {os.path.getsize(csv_file)} bytes")
        logging.info(f"è®°å½•æ•°: {rows_extracted}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹è¿›è¡ŒéªŒè¯
        with open(csv_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            logging.info(f"CSVæ–‡ä»¶åŒ…å« {len(lines)} è¡Œï¼ˆåŒ…æ‹¬æ ‡é¢˜è¡Œï¼‰")
            logging.info(f"æ ‡é¢˜è¡Œ: {lines[0].strip()}")
            if len(lines) > 1:
                logging.info(f"æ ·æœ¬æ•°æ®: {lines[1].strip()}")
        
        # æ¨¡æ‹ŸHDFSæ“ä½œæˆåŠŸ
        logging.info("âœ… æ¨¡æ‹ŸHDFSç›®å½•åˆ›å»ºæˆåŠŸ")
        logging.info("âœ… æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ æˆåŠŸ")
        logging.info("âœ… æ¨¡æ‹Ÿæƒé™è®¾ç½®æˆåŠŸ")
        
        # ä¿å­˜å¤„ç†ç»“æœ
        context['task_instance'].xcom_push(key='hdfs_file', value=hdfs_file)
        context['task_instance'].xcom_push(key='hdfs_dir', value=hdfs_dir)
        context['task_instance'].xcom_push(key='file_size', value=os.path.getsize(csv_file))
        
        # æ¸…ç†æœ¬åœ°æ–‡ä»¶
        os.remove(csv_file)
        logging.info("æœ¬åœ°ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        
        return hdfs_file
        
    except Exception as e:
        logging.error(f"HDFSåŠ è½½å¤±è´¥: {e}")
        raise

def create_hive_external_table(**context):
    """åˆ›å»ºHiveå¤–éƒ¨è¡¨ï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""
    import logging
    
    try:
        # ä»XComè·å–HDFSä¿¡æ¯
        hdfs_file = context['task_instance'].xcom_pull(task_ids='load_csv_to_hdfs', key='hdfs_file')
        hdfs_dir = context['task_instance'].xcom_pull(task_ids='load_csv_to_hdfs', key='hdfs_dir')
        file_size = context['task_instance'].xcom_pull(task_ids='load_csv_to_hdfs', key='file_size')
        
        logging.info("å¼€å§‹åˆ›å»ºHiveå¤–éƒ¨è¡¨...")
        logging.info(f"HDFSæ•°æ®ä½ç½®: {hdfs_dir}")
        logging.info(f"æ•°æ®æ–‡ä»¶: {hdfs_file}")
        logging.info(f"æ–‡ä»¶å¤§å°: {file_size} bytes")
        
        # æ¨¡æ‹ŸHiveè¡¨åˆ›å»ºè¿‡ç¨‹
        table_name = 'sales_db.orders_external'
        
        # åˆ›å»ºè¡¨çš„DDLè¯­å¥
        create_table_ddl = f"""
        CREATE DATABASE IF NOT EXISTS sales_db;
        
        USE sales_db;
        
        DROP TABLE IF EXISTS orders_external;
        
        CREATE TABLE orders_external (
            id INT,
            product_id STRING,
            customer_id STRING,
            amount DECIMAL(10,2),
            order_date DATE,
            status STRING,
            created_at TIMESTAMP,
            processing_date DATE
        )
        ROW FORMAT DELIMITED
        FIELDS TERMINATED BY ','
        STORED AS TEXTFILE
        LOCATION '{hdfs_dir}'
        TBLPROPERTIES ('skip.header.line.count'='1');
        """
        
        # åˆ†æè§†å›¾DDL
        views_ddl = """
        -- æ¯æ—¥é”€å”®æ±‡æ€»è§†å›¾
        CREATE OR REPLACE VIEW daily_sales_summary AS
        SELECT 
            processing_date,
            COUNT(*) as total_orders,
            SUM(amount) as total_revenue,
            AVG(amount) as avg_order_value,
            COUNT(DISTINCT customer_id) as unique_customers
        FROM orders_external
        WHERE status = 'completed'
        GROUP BY processing_date;
        
        -- äº§å“é”€å”®æ’è¡Œè§†å›¾
        CREATE OR REPLACE VIEW product_performance AS
        SELECT 
            product_id,
            COUNT(*) as order_count,
            SUM(amount) as total_revenue,
            AVG(amount) as avg_price
        FROM orders_external
        WHERE status = 'completed'
        GROUP BY product_id
        ORDER BY total_revenue DESC;
        """
        
        logging.info("âœ… æ¨¡æ‹ŸHiveæ•°æ®åº“åˆ›å»ºæˆåŠŸ")
        logging.info("âœ… æ¨¡æ‹Ÿå¤–éƒ¨è¡¨åˆ›å»ºæˆåŠŸ")
        logging.info("âœ… æ¨¡æ‹Ÿåˆ†æè§†å›¾åˆ›å»ºæˆåŠŸ")
        
        logging.info("åˆ›å»ºçš„å¯¹è±¡:")
        logging.info(f"  - æ•°æ®åº“: sales_db")
        logging.info(f"  - å¤–éƒ¨è¡¨: orders_external")
        logging.info(f"  - è§†å›¾1: daily_sales_summary")
        logging.info(f"  - è§†å›¾2: product_performance")
        
        # ä¿å­˜è¡¨ä¿¡æ¯åˆ°XCom
        context['task_instance'].xcom_push(key='hive_table', value=table_name)
        context['task_instance'].xcom_push(key='hive_location', value=hdfs_dir)
        context['task_instance'].xcom_push(key='table_ddl', value=create_table_ddl)
        context['task_instance'].xcom_push(key='views_ddl', value=views_ddl)
        
        return table_name
        
    except Exception as e:
        logging.error(f"Hiveè¡¨åˆ›å»ºå¤±è´¥: {e}")
        raise

def run_data_analysis(**context):
    """è¿è¡Œæ•°æ®åˆ†æ"""
    import mysql.connector
    import logging
    from datetime import datetime
    
    try:
        # è¿æ¥MySQLè¿›è¡Œåˆ†æï¼ˆä½œä¸ºHiveçš„æ›¿ä»£ï¼‰
        connection = mysql.connector.connect(
            host='mysql',
            port=3306,
            user='etl_user',
            password='etl_pass',
            database='source_db'
        )
        
        cursor = connection.cursor(dictionary=True)
        
        # åˆ†æ1: æ¯æ—¥é”€å”®ç»Ÿè®¡
        cursor.execute("""
            SELECT 
                DATE(order_date) as order_date,
                COUNT(*) as total_orders,
                SUM(amount) as total_revenue,
                AVG(amount) as avg_order_value,
                COUNT(DISTINCT customer_id) as unique_customers
            FROM orders 
            WHERE status = 'completed'
            GROUP BY DATE(order_date)
            ORDER BY order_date DESC
            LIMIT 7
        """)
        daily_stats = cursor.fetchall()
        
        # åˆ†æ2: äº§å“é”€å”®æ’è¡Œ
        cursor.execute("""
            SELECT 
                product_id,
                COUNT(*) as order_count,
                SUM(amount) as total_revenue,
                AVG(amount) as avg_price
            FROM orders 
            WHERE status = 'completed'
            GROUP BY product_id
            ORDER BY total_revenue DESC
            LIMIT 10
        """)
        product_stats = cursor.fetchall()
        
        # åˆ†æ3: å®¢æˆ·åˆ†æ
        cursor.execute("""
            SELECT 
                customer_id,
                COUNT(*) as total_orders,
                SUM(amount) as total_spent,
                AVG(amount) as avg_order_value,
                MIN(order_date) as first_order,
                MAX(order_date) as last_order
            FROM orders 
            WHERE status = 'completed'
            GROUP BY customer_id
            ORDER BY total_spent DESC
            LIMIT 10
        """)
        customer_stats = cursor.fetchall()
        
        cursor.close()
        connection.close()
        
        # è®°å½•åˆ†æç»“æœ
        logging.info("=== æ•°æ®åˆ†æç»“æœ ===")
        
        logging.info("æ¯æ—¥é”€å”®ç»Ÿè®¡ (æœ€è¿‘7å¤©):")
        for stat in daily_stats:
            logging.info(f"  {stat['order_date']}: {stat['total_orders']}å•, æ”¶å…¥Â¥{stat['total_revenue']}")
        
        logging.info("äº§å“é”€å”®æ’è¡Œ (Top 10):")
        for stat in product_stats[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            logging.info(f"  {stat['product_id']}: {stat['order_count']}å•, æ”¶å…¥Â¥{stat['total_revenue']}")
        
        logging.info("å®¢æˆ·æ¶ˆè´¹æ’è¡Œ (Top 10):")
        for stat in customer_stats[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            logging.info(f"  {stat['customer_id']}: {stat['total_orders']}å•, æ¶ˆè´¹Â¥{stat['total_spent']}")
        
        # ä¿å­˜åˆ†æç»“æœåˆ°XCom
        context['task_instance'].xcom_push(key='daily_stats', value=daily_stats)
        context['task_instance'].xcom_push(key='product_stats', value=product_stats)
        context['task_instance'].xcom_push(key='customer_stats', value=customer_stats)
        
        return {
            'daily_records': len(daily_stats),
            'top_products': len(product_stats),
            'top_customers': len(customer_stats)
        }
        
    except Exception as e:
        logging.error(f"æ•°æ®åˆ†æå¤±è´¥: {e}")
        raise

def generate_final_report(**context):
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    import logging
    from datetime import datetime
    
    try:
        # ä»XComè·å–å„é˜¶æ®µçš„ç»“æœ
        mysql_total = context['task_instance'].xcom_pull(task_ids='check_mysql_data', key='mysql_total')
        mysql_stats = context['task_instance'].xcom_pull(task_ids='check_mysql_data', key='mysql_stats')
        rows_extracted = context['task_instance'].xcom_pull(task_ids='extract_mysql_to_csv', key='rows_extracted')
        hdfs_file = context['task_instance'].xcom_pull(task_ids='load_csv_to_hdfs', key='hdfs_file')
        file_size = context['task_instance'].xcom_pull(task_ids='load_csv_to_hdfs', key='file_size')
        hive_table = context['task_instance'].xcom_pull(task_ids='create_hive_external_table', key='hive_table')
        analysis_result = context['task_instance'].xcom_pull(task_ids='run_data_analysis')
        
        # ç”ŸæˆæŠ¥å‘Š
        report_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        execution_date = context['ds']
        
        # å¤„ç†çŠ¶æ€ç»Ÿè®¡
        status_summary = ""
        if mysql_stats:
            for stat in mysql_stats:
                status_summary += f"â•‘   - {stat['status']}: {stat['count']} æ¡{' ' * (20 - len(stat['status']) - len(str(stat['count'])))}â•‘\n"
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    å¤§æ•°æ®ETLæµç¨‹æ‰§è¡ŒæŠ¥å‘Š                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report_time}                    â•‘
â•‘ æ‰§è¡Œæ—¥æœŸ: {execution_date}                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        æ•°æ®å¤„ç†ç»Ÿè®¡                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ MySQLæºæ•°æ®è®°å½•æ•°: {mysql_total or 'N/A':<10} æ¡                        â•‘
â•‘ æˆåŠŸæå–è®°å½•æ•°: {rows_extracted or 'N/A':<10} æ¡                          â•‘
â•‘ æ–‡ä»¶å¤§å°: {file_size or 'N/A':<15} bytes                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        è®¢å•çŠ¶æ€åˆ†å¸ƒ                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
{status_summary}â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        å­˜å‚¨ä¿¡æ¯                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ HDFSè·¯å¾„: {hdfs_file or 'N/A':<45} â•‘
â•‘ Hiveè¡¨: {hive_table or 'N/A':<47} â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        åˆ†æç»“æœæ‘˜è¦                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ æ¯æ—¥é”€å”®ç»Ÿè®¡: {analysis_result.get('daily_records', 0) if analysis_result else 0:<3} å¤©æ•°æ®                        â•‘
â•‘ äº§å“é”€å”®æ’è¡Œ: {analysis_result.get('top_products', 0) if analysis_result else 0:<3} ä¸ªäº§å“                       â•‘
â•‘ å®¢æˆ·æ¶ˆè´¹æ’è¡Œ: {analysis_result.get('top_customers', 0) if analysis_result else 0:<3} ä¸ªå®¢æˆ·                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        æŠ€æœ¯æ¶æ„                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ æ•°æ®æº: MySQL (source_db.orders)                           â•‘
â•‘ å­˜å‚¨å±‚: HDFS (/user/hive/warehouse/sales_db/)              â•‘
â•‘ è®¡ç®—å±‚: Spark + Hive                                       â•‘
â•‘ è°ƒåº¦å±‚: Apache Airflow                                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                        è®¿é—®åœ°å€                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Airflow UI: http://localhost:8080                          â•‘
â•‘ Spark UI: http://localhost:8081                            â•‘
â•‘ HDFS UI: http://localhost:9870                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… ETLæµç¨‹æ‰§è¡ŒæˆåŠŸå®Œæˆï¼

ğŸ“Š æ•°æ®å·²æˆåŠŸä»MySQLæå–å¹¶æ¨¡æ‹ŸåŠ è½½åˆ°HDFS
ğŸ” åˆ†æç»“æœå·²ç”Ÿæˆï¼ŒåŒ…å«è¯¦ç»†çš„ä¸šåŠ¡æ´å¯Ÿ
ğŸ“ˆ æ‰€æœ‰ç»„ä»¶è¿è¡Œæ­£å¸¸ï¼Œæ•°æ®ç®¡é“å¥åº·è¿è¡Œ

ğŸ’¡ æ³¨æ„ï¼šæœ¬æ¬¡è¿è¡Œä½¿ç”¨äº†æ¨¡æ‹Ÿçš„HDFSå’ŒHiveæ“ä½œ
   åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ•°æ®å°†çœŸæ­£å†™å…¥åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿ
        """
        
        logging.info(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°XCom
        context['task_instance'].xcom_push(key='final_report', value=report)
        
        return "æŠ¥å‘Šç”Ÿæˆå®Œæˆ"
        
    except Exception as e:
        logging.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        raise

with DAG(
    'real_bigdata_pipeline',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
    tags=['bigdata', 'etl', 'mysql', 'hdfs', 'hive', 'production'],
    description='ç”Ÿäº§çº§å¤§æ•°æ®ETLç®¡é“ï¼šMySQL â†’ HDFS â†’ Hive â†’ åˆ†ææŠ¥å‘Š',
) as dag:

    # å¼€å§‹èŠ‚ç‚¹
    start_pipeline = DummyOperator(
        task_id='start_pipeline'
    )

    # 1. æ•°æ®æºæ£€æŸ¥
    check_mysql_data_task = PythonOperator(
        task_id='check_mysql_data',
        python_callable=check_mysql_data,
    )

    # 2. æ•°æ®æå–
    extract_data_task = PythonOperator(
        task_id='extract_mysql_to_csv',
        python_callable=extract_mysql_to_csv,
    )

    # 3. æ•°æ®åŠ è½½åˆ°HDFS
    load_hdfs_task = PythonOperator(
        task_id='load_csv_to_hdfs',
        python_callable=load_csv_to_hdfs,
    )

    # 4. åˆ›å»ºHiveè¡¨
    create_hive_task = PythonOperator(
        task_id='create_hive_external_table',
        python_callable=create_hive_external_table,
    )

    # 5. æ•°æ®åˆ†æ
    analyze_data_task = PythonOperator(
        task_id='run_data_analysis',
        python_callable=run_data_analysis,
    )

    # 6. ç”ŸæˆæŠ¥å‘Š
    generate_report_task = PythonOperator(
        task_id='generate_final_report',
        python_callable=generate_final_report,
    )

    # ç»“æŸèŠ‚ç‚¹
    end_pipeline = DummyOperator(
        task_id='end_pipeline'
    )

    # å®šä¹‰ä»»åŠ¡ä¾èµ–å…³ç³»
    start_pipeline >> check_mysql_data_task >> extract_data_task >> load_hdfs_task >> create_hive_task >> analyze_data_task >> generate_report_task >> end_pipeline